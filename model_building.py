# -*- coding: utf-8 -*-
"""model_building_cross_val.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1koP18wilCaYP-czi5znffIbc5qdJsAK1

# **Import Library**
"""

!apt update
!yes Y | apt-get install openjdk-8-jdk

!pip install language-check
!pip install pycontractions
!pip install word2number
!pip install vaderSentiment
!pip install textaugment
!pip install wordcloud

import itertools
import tweepy
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

pd.set_option('display.max_colwidth', None)

import re
import string
import nltk
import unicodedata
from bs4 import BeautifulSoup
from word2number import w2n
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from pycontractions import Contractions
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textaugment import EDA

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

import gensim
import tensorflow as tf
from gensim.models import Word2Vec
from gensim.scripts.glove2word2vec import glove2word2vec
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical, plot_model
from keras.models import Model
from keras.layers import (Input,
                          Embedding, 
                          Conv1D, 
                          MaxPooling1D, 
                          LSTM, 
                          Dropout, 
                          Dense, 
                          Flatten)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import regularizers
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold

!pip freeze > requirement.txt

"""# **Crawling Twitter Dataset**"""

ACCESS_TOKEN = ''
ACCESS_TOKEN_SECRET = ''
CONSUMER_KEY = ''
CONSUMER_SECRET = ''

auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)

search_sinovac_keywords = '(sinovac OR #sinovac) -moderna -pfizer -astrazeneca lang:en -is:retweet'
search_astrazeneca_keywords = '(astrazeneca OR #astrazeneca) -moderna -pfizer -sinovac lang:en -is:retweet'
search_pfizer_keywords = '(pfizer OR #pfizer) -sinovac -moderna -astrazeneca lang:en -is:retweet'
search_moderna_keywords = '(moderna OR #moderna) -sinovac -pfizer -astrazeneca lang:en -is:retweet'

tweets = tweepy.Cursor(api.search, q=search_astrazeneca_keywords, tweet_mode='extended', lang='en').items(2000)

tweet_texts = []
tweet_dates = []
tweet_langs = []
for tweet in tweets:
    tweet_texts.append(tweet.full_text)
    tweet_dates.append(tweet.lang)
    tweet_langs.append(tweet.created_at)

df = pd.DataFrame({'text': tweets_text, 'lang': tweets_lang, 'date': tweets_date})
df.head()

df.shape

df.to_csv('astrazeneca42.csv', index=False)

"""# **Concat and Read Dataset**

## **Concate Dataset: Sinovac**
"""

df_sinovac1 = pd.read_csv('raw/sinovac1.csv')
df_sinovac2 = pd.read_csv('raw/sinovac2.csv')
df_sinovac3 = pd.read_csv('raw/sinovac3.csv')
df_sinovac41 = pd.read_csv('raw/sinovac41.csv')
df_sinovac42 = pd.read_csv('raw/sinovac42.csv')

df_sinovac = pd.concat([df_sinovac1, df_sinovac2, df_sinovac3, df_sinovac41, df_sinovac42], ignore_index=True)

"""## **Concate Dataset: AstraZeneca**"""

df_astrazeneca1 = pd.read_csv('raw/astrazeneca1.csv')
df_astrazeneca2 = pd.read_csv('raw/astrazeneca2.csv')
df_astrazeneca3 = pd.read_csv('raw/astrazeneca3.csv')
df_astrazeneca41 = pd.read_csv('raw/astrazeneca41.csv')
df_astrazeneca42 = pd.read_csv('raw/astrazeneca42.csv')

df_astrazeneca = pd.concat([df_astrazeneca1, df_astrazeneca2, df_astrazeneca3, df_astrazeneca41, df_astrazeneca42], ignore_index=True)

"""## **Concate Dataset: Pfizer**"""

df_pfizer1 = pd.read_csv('raw/pfizer1.csv')
df_pfizer2 = pd.read_csv('raw/pfizer2.csv')
df_pfizer3 = pd.read_csv('raw/pfizer3.csv')
df_pfizer41 = pd.read_csv('raw/pfizer41.csv')
df_pfizer42 = pd.read_csv('raw/pfizer42.csv')

df_pfizer = pd.concat([df_pfizer1, df_pfizer2, df_pfizer3, df_pfizer41, df_pfizer42], ignore_index=True)

"""## **Concate Dataset: Moderna**"""

df_moderna1 = pd.read_csv('raw/moderna1.csv')
df_moderna2 = pd.read_csv('raw/moderna2.csv')
df_moderna3 = pd.read_csv('raw/moderna3.csv')
df_moderna41 = pd.read_csv('raw/moderna41.csv')
df_moderna42 = pd.read_csv('raw/moderna42.csv')

df_moderna = pd.concat([df_moderna1, df_moderna2, df_moderna3, df_moderna41, df_moderna42], ignore_index=True)

df_sinovac.to_csv('merged/sinovac_merged.csv', index=False)
df_astrazeneca.to_csv('merged/astrazeneca_merged.csv', index=False)
df_pfizer.to_csv('merged/pfizer_merged.csv', index=False)
df_moderna.to_csv('merged/moderna_merged.csv', index=False)

"""# **Read and Filter Dataset**

## **Read and Filter Dataset: Sinovac**
"""

df_sinovac = pd.read_csv('merged/sinovac_merged.csv')

df_sinovac.head(5)

df_sinovac.dtypes

before_filtered = df_sinovac.shape
print('Shape before filtered : ', before_filtered)

start_date_before_filtered = min(df_sinovac['date'])
end_date_before_filtered = max(df_sinovac['date'])

print('Start date : ', start_date_before_filtered)
print('End date : ', end_date_before_filtered)

# Filter only March
df_sinovac = df_sinovac.drop(df_sinovac[df_sinovac['date'] < '2022-03-01 00:00:00'].index)
df_sinovac = df_sinovac.drop(df_sinovac[df_sinovac['date'] > '2022-03-31 24:59:59'].index)
df_sinovac = df_sinovac.drop(df_sinovac[df_sinovac['lang'] != 'en'].index)
df_sinovac = df_sinovac.reset_index(drop=True)

after_filtered = df_sinovac.shape
print('Shape after filtered : ', after_filtered)

start_date_after_filtered = min(df_sinovac['date'])
end_date_after_filtered = max(df_sinovac['date'])


print('Start date after filtered : ', start_date_after_filtered)
print('End date after filtered : ', end_date_after_filtered)

"""## **Read and Filter Dataset: AstraZeneca**"""

df_astrazeneca = pd.read_csv('merged/astrazeneca_merged.csv')

df_astrazeneca.head(5)

df_astrazeneca.dtypes

before_filtered = df_astrazeneca.shape
print('Shape before filtered : ', before_filtered)

start_date_before_filtered = min(df_astrazeneca['date'])
end_date_before_filtered = max(df_astrazeneca['date'])

print('Start date : ', start_date_before_filtered)
print('End date : ', end_date_before_filtered)

# Filter only March
df_astrazeneca = df_astrazeneca.drop(df_astrazeneca[df_astrazeneca['date'] < '2022-03-01 00:00:00'].index)
df_astrazeneca = df_astrazeneca.drop(df_astrazeneca[df_astrazeneca['date'] > '2022-03-31 24:59:59'].index)
df_astrazeneca = df_astrazeneca.drop(df_astrazeneca[df_astrazeneca['lang'] != 'en'].index)
df_astrazeneca = df_astrazeneca.reset_index(drop=True)

after_filtered = df_astrazeneca.shape
print('Shape after filtered : ', after_filtered)

start_date_after_filtered = min(df_astrazeneca['date'])
end_date_after_filtered = max(df_astrazeneca['date'])


print('Start date after filtered : ', start_date_after_filtered)
print('End date after filtered : ', end_date_after_filtered)

"""## **Read and Filter Dataset: Pfizer**"""

df_pfizer = pd.read_csv('merged/pfizer_merged.csv')

df_pfizer.head(5)

df_pfizer.dtypes

before_filtered = df_pfizer.shape
print('Shape before filtered : ', before_filtered)

start_date_before_filtered = min(df_pfizer['date'])
end_date_before_filtered = max(df_pfizer['date'])

print('Start date : ', start_date_before_filtered)
print('End date : ', end_date_before_filtered)

# Filter only March
df_pfizer = df_pfizer.drop(df_pfizer[df_pfizer['date'] < '2022-03-01 00:00:00'].index)
df_pfizer = df_pfizer.drop(df_pfizer[df_pfizer['date'] > '2022-03-31 24:59:59'].index)
df_pfizer = df_pfizer.drop(df_pfizer[df_pfizer['lang'] != 'en'].index)
df_pfizer = df_pfizer.reset_index(drop=True)

after_filtered = df_pfizer.shape
print('Shape after filtered : ', after_filtered)

start_date_after_filtered = min(df_pfizer['date'])
end_date_after_filtered = max(df_pfizer['date'])


print('Start date after filtered : ', start_date_after_filtered)
print('End date after filtered : ', end_date_after_filtered)

"""## **Read and Filter Dataset: Moderna**"""

df_moderna = pd.read_csv('merged/moderna_merged.csv')

df_moderna.head(5)

df_moderna.dtypes

before_filtered = df_moderna.shape
print('Shape before filtered : ', before_filtered)

start_date_before_filtered = min(df_moderna['date'])
end_date_before_filtered = max(df_moderna['date'])

print('Start date : ', start_date_before_filtered)
print('End date : ', end_date_before_filtered)

# Filter only March
df_moderna = df_moderna.drop(df_moderna[df_moderna['date'] < '2022-03-01 00:00:00'].index)
df_moderna = df_moderna.drop(df_moderna[df_moderna['date'] > '2022-03-31 24:59:59'].index)
df_moderna = df_moderna.drop(df_moderna[df_moderna['lang'] != 'en'].index)
df_moderna = df_moderna.reset_index(drop=True)

after_filtered = df_moderna.shape
print('Shape after filtered : ', after_filtered)

start_date_after_filtered = min(df_moderna['date'])
end_date_after_filtered = max(df_moderna['date'])


print('Start date after filtered : ', start_date_after_filtered)
print('End date after filtered : ', end_date_after_filtered)

df_sinovac.to_csv('filtered/sinovac_filtered.csv', index=False)
df_astrazeneca.to_csv('filtered/astrazeneca_filtered.csv', index=False)
df_pfizer.to_csv('filtered/pfizer_filtered.csv', index=False)
df_moderna.to_csv('filtered/moderna_filtered.csv', index=False)

"""# **Clean and Pre-Process Dataset**"""

# Pre-load pycontractions pre-trained model
contractions_expander = Contractions('GoogleNews-vectors-negative300.bin')
contractions_expander.load_models()

class TweetTextCleaner:
    # Remove html tags in text
    def remove_html_tags(self, text):
        cleaned_text = BeautifulSoup(text, 'lxml')
        return cleaned_text.get_text()
    
     # Remove 'RT' or 'rt' in text
    def remove_retweets(self, text):
        cleaned_text = re.sub(r'\bRT\b', '', text)
        return cleaned_text

    # Remove URLs in text
    def remove_urls(self, text):
        cleaned_text = re.sub('(?:http?\:\/\/|http?\:\/|http?\:|https?\:\/\/|https?\:\/|https?\:|www)\S+', '', text)
        return cleaned_text

    # Remove username with '@' in text
    def remove_mentions(self, text):
        cleaned_text = re.sub('@[^\s]+', '', text)
        return cleaned_text

    # Remove hashtags '#' in text
    def remove_hashtags(self, text):
        cleaned_text = re.sub('#\w+\s*', '', text)
        return cleaned_text

    # Normalized the accented character in text
    def remove_non_ascii(self, text):
        cleaned_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        # Remove turncate symbol
        cleaned_text = re.sub('[\.]{3,}', '', cleaned_text)
        return cleaned_text

    # Change the written number in word to actual number in text
    def change_word_to_number(self, text):
        changed_text = []
        for word in text.split():
            try:
                changed_text += [str(w2n.word_to_num(word))]
            except ValueError:
                changed_text += [word] 
        changed_text = ' '.join(changed_text)
        return changed_text

    # Remove numbers in text
    def remove_numbers(self, text):
        cleaned_text = ''.join([word for word in text if not word.isdigit()])  
        return cleaned_text
    
    # Lower case all words in text
    def case_folding(self, text):
        return text.lower()

    # Expand the existing contractions in text
    def expand_contractions(self, text, contractions_expander):
        expanded_text = list(contractions_expander.expand_texts([text], precise=True))
        expanded_text = ' '.join(expanded_text)
        return expanded_text

    # Get the correspond antonym to the given word
    def get_antonym(self, word):
        word_antonyms = set()
        # Get the synsets to the given word
        for syn in wordnet.synsets(word):
            # Get the correspond lemmas to the given word
            for lemma in syn.lemmas():
                # Get all the antonyms to the given word
                for antonym in lemma.antonyms():
                    word_antonyms.add(antonym.name())

        # Get the relevant antonym
        if len(word_antonyms) != 0:
            return word_antonyms.pop()
        else:
            return None

    # Replace the negation words with the antonym in text
    def replace_negation(self, text):
        replaced_text = []
        index = 0
        tokenized_text = self.tokenize(text)
        token_length = len(tokenized_text)
        while index < token_length:
            current_word = tokenized_text[index]
            # Check if word is negation
            if (current_word == 'not' or current_word == 'no') and index+1 < token_length:
                current_antonym = self.get_antonym(tokenized_text[index+1])
                
                # Replace if negation word
                if current_antonym:
                    # Store the replaced negation word with the antonym
                    replaced_text.append(current_antonym)
                    index += 2
                    continue
            # Store the non-negation word
            replaced_text.append(current_word)
            index += 1

        replaced_text = ' '.join(replaced_text)

        return replaced_text
    
    # Remove punctuations in text
    def remove_punctuations(self, text):
        english_punctuations = string.punctuation
        translator = str.maketrans('', '', english_punctuations)
        cleaned_text = text.translate(translator)
        return cleaned_text

    # Tokenize sentence into word
    def tokenize(self, text):
        tokenized_text = word_tokenize(text) 
        return tokenized_text

    # Remove stopwords in text
    def remove_stopwords(self, text):
        stop_words = stopwords.words('english')
        cleaned_text =  [word for word in self.tokenize(text) if word not in stop_words]
        cleaned_text = ' '.join(cleaned_text)
        return cleaned_text

    # Get the correspond word tag
    def get_pos_tag(self,word):
        current_word_tag = nltk.pos_tag([word])[0][1][0].upper()
        tag_map = {'J': wordnet.ADJ,
                   'N': wordnet.NOUN,
                   'V': wordnet.VERB,
                   'R': wordnet.ADV}

        return tag_map.get(current_word_tag, wordnet.NOUN)

    # Lemmatize each word in text
    def lemmatize(self, text):
        lemmatizer = WordNetLemmatizer()
        lematized_text = [lemmatizer.lemmatize(word, self.get_pos_tag(word)) for word in self.tokenize(text)]
        lematized_text = [word for word in lematized_text if len(word) > 2]
        lematized_text = ' '.join(lematized_text)
        return lematized_text

text_cleaner = TweetTextCleaner()

df_sinovac = pd.read_csv('filtered/sinovac_filtered.csv')
df_astrazeneca = pd.read_csv('filtered/astrazeneca_filtered.csv')
df_pfizer = pd.read_csv('filtered/pfizer_filtered.csv')
df_moderna = pd.read_csv('filtered/moderna_filtered.csv')

"""## **Sinovac Dataset Pre-Process**"""

df_sinovac['removed_html_tags'] = df_sinovac['text'].apply(lambda sentiment: text_cleaner.remove_html_tags(sentiment))
df_sinovac['removed_retweets'] = df_sinovac['removed_html_tags'].apply(lambda sentiment: text_cleaner.remove_retweets(sentiment))
df_sinovac['removed_urls'] = df_sinovac['removed_retweets'].apply(lambda sentiment: text_cleaner.remove_urls(sentiment))
df_sinovac['removed_mentions'] = df_sinovac['removed_urls'].apply(lambda sentiment: text_cleaner.remove_mentions(sentiment))
df_sinovac['removed_hashtags'] = df_sinovac['removed_mentions'].apply(lambda sentiment: text_cleaner.remove_hashtags(sentiment))
df_sinovac['removed_non_ascii'] = df_sinovac['removed_hashtags'].apply(lambda sentiment: text_cleaner.remove_non_ascii(sentiment))
df_sinovac['changed_word_to_number'] = df_sinovac['removed_non_ascii'].apply(lambda sentiment: text_cleaner.change_word_to_number(sentiment))
df_sinovac['removed_numbers'] = df_sinovac['changed_word_to_number'].apply(lambda sentiment: text_cleaner.remove_numbers(sentiment))
df_sinovac['case_folding'] = df_sinovac['removed_numbers'].apply(lambda sentiment: text_cleaner.case_folding(sentiment))
df_sinovac['expanded_contractions'] = df_sinovac['case_folding'].apply(lambda sentiment: text_cleaner.expand_contractions(sentiment, contractions_expander))
df_sinovac['replaced_negation'] = df_sinovac['expanded_contractions'].apply(lambda sentiment: text_cleaner.replace_negation(sentiment))
df_sinovac['removed_punctuations'] = df_sinovac['replaced_negation'].apply(lambda sentiment: text_cleaner.remove_punctuations(sentiment))
df_sinovac['removed_stopwords'] = df_sinovac['removed_punctuations'].apply(lambda sentiment: text_cleaner.remove_stopwords(sentiment))
df_sinovac['lemmatized'] = df_sinovac['removed_stopwords'].apply(lambda sentiment: text_cleaner.lemmatize(sentiment))

df_sinovac.head()

"""## **AstraZeneca Dataset Pre-Process**"""

df_astrazeneca['removed_html_tags'] = df_astrazeneca['text'].apply(lambda sentiment: text_cleaner.remove_html_tags(sentiment))
df_astrazeneca['removed_retweets'] = df_astrazeneca['removed_html_tags'].apply(lambda sentiment: text_cleaner.remove_retweets(sentiment))
df_astrazeneca['removed_urls'] = df_astrazeneca['removed_retweets'].apply(lambda sentiment: text_cleaner.remove_urls(sentiment))
df_astrazeneca['removed_mentions'] = df_astrazeneca['removed_urls'].apply(lambda sentiment: text_cleaner.remove_mentions(sentiment))
df_astrazeneca['removed_hashtags'] = df_astrazeneca['removed_mentions'].apply(lambda sentiment: text_cleaner.remove_hashtags(sentiment))
df_astrazeneca['removed_non_ascii'] = df_astrazeneca['removed_hashtags'].apply(lambda sentiment: text_cleaner.remove_non_ascii(sentiment))
df_astrazeneca['changed_word_to_number'] = df_astrazeneca['removed_non_ascii'].apply(lambda sentiment: text_cleaner.change_word_to_number(sentiment))
df_astrazeneca['removed_numbers'] = df_astrazeneca['changed_word_to_number'].apply(lambda sentiment: text_cleaner.remove_numbers(sentiment))
df_astrazeneca['case_folding'] = df_astrazeneca['removed_numbers'].apply(lambda sentiment: text_cleaner.case_folding(sentiment))
df_astrazeneca['expanded_contractions'] = df_astrazeneca['case_folding'].apply(lambda sentiment: text_cleaner.expand_contractions(sentiment, contractions_expander))
df_astrazeneca['replaced_negation'] = df_astrazeneca['expanded_contractions'].apply(lambda sentiment: text_cleaner.replace_negation(sentiment))
df_astrazeneca['removed_punctuations'] = df_astrazeneca['replaced_negation'].apply(lambda sentiment: text_cleaner.remove_punctuations(sentiment))
df_astrazeneca['removed_stopwords'] = df_astrazeneca['removed_punctuations'].apply(lambda sentiment: text_cleaner.remove_stopwords(sentiment))
df_astrazeneca['lemmatized'] = df_astrazeneca['removed_stopwords'].apply(lambda sentiment: text_cleaner.lemmatize(sentiment))

df_astrazeneca.head()

"""## **Pfizer Dataset Pre-Process**"""

df_pfizer['removed_html_tags'] = df_pfizer['text'].apply(lambda sentiment: text_cleaner.remove_html_tags(sentiment))
df_pfizer['removed_retweets'] = df_pfizer['removed_html_tags'].apply(lambda sentiment: text_cleaner.remove_retweets(sentiment))
df_pfizer['removed_urls'] = df_pfizer['removed_retweets'].apply(lambda sentiment: text_cleaner.remove_urls(sentiment))
df_pfizer['removed_mentions'] = df_pfizer['removed_urls'].apply(lambda sentiment: text_cleaner.remove_mentions(sentiment))
df_pfizer['removed_hashtags'] = df_pfizer['removed_mentions'].apply(lambda sentiment: text_cleaner.remove_hashtags(sentiment))
df_pfizer['removed_non_ascii'] = df_pfizer['removed_hashtags'].apply(lambda sentiment: text_cleaner.remove_non_ascii(sentiment))
df_pfizer['changed_word_to_number'] = df_pfizer['removed_non_ascii'].apply(lambda sentiment: text_cleaner.change_word_to_number(sentiment))
df_pfizer['removed_numbers'] = df_pfizer['changed_word_to_number'].apply(lambda sentiment: text_cleaner.remove_numbers(sentiment))
df_pfizer['case_folding'] = df_pfizer['removed_numbers'].apply(lambda sentiment: text_cleaner.case_folding(sentiment))
df_pfizer['expanded_contractions'] = df_pfizer['case_folding'].apply(lambda sentiment: text_cleaner.expand_contractions(sentiment, contractions_expander))
df_pfizer['replaced_negation'] = df_pfizer['expanded_contractions'].apply(lambda sentiment: text_cleaner.replace_negation(sentiment))
df_pfizer['removed_punctuations'] = df_pfizer['replaced_negation'].apply(lambda sentiment: text_cleaner.remove_punctuations(sentiment))
df_pfizer['removed_stopwords'] = df_pfizer['removed_punctuations'].apply(lambda sentiment: text_cleaner.remove_stopwords(sentiment))
df_pfizer['lemmatized'] = df_pfizer['removed_stopwords'].apply(lambda sentiment: text_cleaner.lemmatize(sentiment))

df_pfizer.head()

"""## **Moderna Dataset Pre-Process**"""

df_moderna['removed_html_tags'] = df_moderna['text'].apply(lambda sentiment: text_cleaner.remove_html_tags(sentiment))
df_moderna['removed_retweets'] = df_moderna['removed_html_tags'].apply(lambda sentiment: text_cleaner.remove_retweets(sentiment))
df_moderna['removed_urls'] = df_moderna['removed_retweets'].apply(lambda sentiment: text_cleaner.remove_urls(sentiment))
df_moderna['removed_mentions'] = df_moderna['removed_urls'].apply(lambda sentiment: text_cleaner.remove_mentions(sentiment))
df_moderna['removed_hashtags'] = df_moderna['removed_mentions'].apply(lambda sentiment: text_cleaner.remove_hashtags(sentiment))
df_moderna['removed_non_ascii'] = df_moderna['removed_hashtags'].apply(lambda sentiment: text_cleaner.remove_non_ascii(sentiment))
df_moderna['changed_word_to_number'] = df_moderna['removed_non_ascii'].apply(lambda sentiment: text_cleaner.change_word_to_number(sentiment))
df_moderna['removed_numbers'] = df_moderna['changed_word_to_number'].apply(lambda sentiment: text_cleaner.remove_numbers(sentiment))
df_moderna['case_folding'] = df_moderna['removed_numbers'].apply(lambda sentiment: text_cleaner.case_folding(sentiment))
df_moderna['expanded_contractions'] = df_moderna['case_folding'].apply(lambda sentiment: text_cleaner.expand_contractions(sentiment, contractions_expander))
df_moderna['replaced_negation'] = df_moderna['expanded_contractions'].apply(lambda sentiment: text_cleaner.replace_negation(sentiment))
df_moderna['removed_punctuations'] = df_moderna['replaced_negation'].apply(lambda sentiment: text_cleaner.remove_punctuations(sentiment))
df_moderna['removed_stopwords'] = df_moderna['removed_punctuations'].apply(lambda sentiment: text_cleaner.remove_stopwords(sentiment))
df_moderna['lemmatized'] = df_moderna['removed_stopwords'].apply(lambda sentiment: text_cleaner.lemmatize(sentiment))

df_moderna.head()

"""## **Remove Duplicate Dataset After Pre-Processing**"""

# Before remove duplicated tweets
print('Sinovac dataset before removed duplicates : ', df_sinovac.shape)
print('Astrazeneca dataset before removed duplicates : ', df_astrazeneca.shape)
print('Pfizer dataset before removed duplicates : ', df_pfizer.shape)
print('Moderna dataset before removed duplicates : ', df_moderna.shape)

# Remove duplicated tweets
df_sinovac_unique = df_sinovac.drop_duplicates(['lemmatized'], keep='first', ignore_index=True)
df_astrazeneca_unique = df_astrazeneca.drop_duplicates(['lemmatized'], keep='first', ignore_index=True)
df_pfizer_unique = df_pfizer.drop_duplicates(['lemmatized'], keep='first', ignore_index=True)
df_moderna_unique = df_moderna.drop_duplicates(['lemmatized'], keep='first', ignore_index=True)

# After remove duplicated tweets
print('Sinovac dataset after removed duplicates : ', df_sinovac_unique.shape)
print('Astrazeneca dataset after removed duplicates : ', df_astrazeneca_unique.shape)
print('Pfizer dataset after removed duplicates : ', df_pfizer_unique.shape)
print('Moderna dataset after removed duplicates : ', df_moderna_unique.shape)

# Re-assign to temp dataframe
df_sinovac_temp = df_sinovac_unique
df_astrazeneca_temp = df_astrazeneca_unique
df_pfizer_temp = df_pfizer_unique
df_moderna_temp = df_moderna_unique

# Filter only first three weeks on March to be used for the sentiment analysis model (experiment data)
df_sinovac = df_sinovac_temp.drop(df_sinovac_temp[df_sinovac_temp['date'] > '2022-03-22 00:00:00'].index).reset_index(drop=True)
df_astrazeneca = df_astrazeneca_temp.drop(df_astrazeneca_temp[df_astrazeneca_temp['date'] > '2022-03-22 00:00:00'].index).reset_index(drop=True)
df_pfizer = df_pfizer_temp.drop(df_pfizer_temp[df_pfizer_temp['date'] > '2022-03-22 00:00:00'].index).reset_index(drop=True)
df_moderna = df_moderna_temp.drop(df_moderna_temp[df_moderna_temp['date'] > '2022-03-22 00:00:00'].index).reset_index(drop=True)

# Filter only last one weeks on March to be used for the sentiment analysis application (implementation data)
df_sinovac_app = df_sinovac_temp.drop(df_sinovac_temp[df_sinovac_temp['date'] < '2022-03-22 00:00:00'].index).reset_index(drop=True)
df_astrazeneca_app = df_astrazeneca_temp.drop(df_astrazeneca_temp[df_astrazeneca_temp['date'] < '2022-03-22 00:00:00'].index).reset_index(drop=True)
df_pfizer_app = df_pfizer_temp.drop(df_pfizer_temp[df_pfizer_temp['date'] < '2022-03-22 00:00:00'].index).reset_index(drop=True)
df_moderna_app = df_moderna_temp.drop(df_moderna_temp[df_moderna_temp['date'] < '2022-03-22 00:00:00'].index).reset_index(drop=True)

print('Sinovac dataset first three weeks : ', df_sinovac.shape)
print('Astrazeneca dataset first three weeks : ', df_astrazeneca.shape)
print('Pfizer dataset first three weeks : ', df_pfizer.shape)
print('Moderna dataset first three weeks : ', df_moderna.shape)

print('\nSinovac dataset last one weeks : ', df_sinovac_app.shape)
print('Astrazeneca dataset last one weeks : ', df_astrazeneca_app.shape)
print('Pfizer dataset last one weeks : ', df_pfizer_app.shape)
print('Moderna dataset last one weeks : ', df_moderna_app.shape)

df_sinovac = df_sinovac[df_sinovac['lemmatized'].astype(bool) == True]
df_astrazeneca = df_astrazeneca[df_astrazeneca['lemmatized'].astype(bool) == True]
df_pfizer = df_pfizer[df_pfizer['lemmatized'].astype(bool) == True]
df_moderna = df_moderna[df_moderna['lemmatized'].astype(bool) == True]

df_sinovac_app = df_sinovac_app[df_sinovac_app['lemmatized'].astype(bool) == True]
df_astrazeneca_app = df_astrazeneca_app[df_astrazeneca_app['lemmatized'].astype(bool) == True]
df_pfizer_app = df_pfizer_app[df_pfizer_app['lemmatized'].astype(bool) == True]
df_moderna_app = df_moderna_app[df_moderna_app['lemmatized'].astype(bool) == True]

print('Sinovac dataset first three weeks after drop empty : ', df_sinovac.shape)
print('Astrazeneca dataset first three weeks after drop empty : ', df_astrazeneca.shape)
print('Pfizer dataset first three weeks after drop empty : ', df_pfizer.shape)
print('Moderna dataset first three weeks after drop empty : ', df_moderna.shape)

print('\nSinovac dataset last one weeks after drop empty: ', df_sinovac_app.shape)
print('Astrazeneca dataset last one weeks after drop empty : ', df_astrazeneca_app.shape)
print('Pfizer dataset last one weeks after drop empty : ', df_pfizer_app.shape)
print('Moderna dataset last one weeks after drop empty : ', df_moderna_app.shape)

# Remove unessecary column
df_sinovac = df_sinovac.drop(columns=['lang', 'date'])
df_astrazeneca = df_astrazeneca.drop(columns=['lang', 'date'])
df_pfizer = df_pfizer.drop(columns=['lang', 'date'])
df_moderna = df_moderna.drop(columns=['lang', 'date'])

# Add sentiment column to the implementation data
df_sinovac_app = pd.DataFrame({'text': df_sinovac_app['text']})
df_sinovac_app['sentiment'] = ''

df_astrazeneca_app = pd.DataFrame({'text': df_astrazeneca_app['text']})
df_astrazeneca_app['sentiment'] = ''

df_pfizer_app = pd.DataFrame({'text': df_pfizer_app['text']})
df_pfizer_app['sentiment'] = ''

df_moderna_app = pd.DataFrame({'text': df_moderna_app['text']})
df_moderna_app['sentiment'] = ''

df_sinovac.to_csv('cleaned/sinovac_cleaned.csv', index=False)
df_astrazeneca.to_csv('cleaned/astrazeneca_cleaned.csv', index=False)
df_pfizer.to_csv('cleaned/pfizer_cleaned.csv', index=False)
df_moderna.to_csv('cleaned/moderna_cleaned.csv', index=False)

df_sinovac_app.to_csv('implementation/sinovac_app.csv', index=False)
df_astrazeneca_app.to_csv('implementation/astrazeneca_app.csv', index=False)
df_pfizer_app.to_csv('implementation/pfizer_app.csv', index=False)
df_moderna_app.to_csv('implementation/moderna_app.csv', index=False)

"""# **Dataset Labeling**"""

def get_sentiment_label(text):
    vader = SentimentIntensityAnalyzer()
    vader_result = vader.polarity_scores(text)

    if vader_result['compound'] >= 0.05:
        labeled_sentiment = 'positive'
    elif vader_result['compound'] > -0.05 and vader_result['compound'] < 0.05:
        labeled_sentiment = 'neutral'
    else:
        labeled_sentiment = 'negative'

    return labeled_sentiment

df_sinovac = pd.read_csv('cleaned/sinovac_cleaned.csv')
df_astrazeneca = pd.read_csv('cleaned/astrazeneca_cleaned.csv')
df_pfizer = pd.read_csv('cleaned/pfizer_cleaned.csv')
df_moderna = pd.read_csv('cleaned/moderna_cleaned.csv')

# Applying VADER sentiment label
df_sinovac['sentiment'] = df_sinovac['removed_hashtags'].apply(lambda sentiment: get_sentiment_label(sentiment))
df_sinovac = df_sinovac[df_sinovac['sentiment'] != 'neutral']
df_sinovac = df_sinovac[['lemmatized', 'sentiment']].rename(columns={'lemmatized': 'text'})

df_astrazeneca['sentiment'] = df_astrazeneca['removed_hashtags'].apply(lambda sentiment: get_sentiment_label(sentiment))
df_astrazeneca = df_astrazeneca[df_astrazeneca['sentiment'] != 'neutral']
df_astrazeneca = df_astrazeneca[['lemmatized', 'sentiment']].rename(columns={'lemmatized': 'text'})

df_pfizer['sentiment'] = df_pfizer['removed_hashtags'].apply(lambda sentiment: get_sentiment_label(sentiment))
df_pfizer = df_pfizer[df_pfizer['sentiment'] != 'neutral']
df_pfizer = df_pfizer[['lemmatized', 'sentiment']].rename(columns={'lemmatized': 'text'})

df_moderna['sentiment'] = df_moderna['removed_hashtags'].apply(lambda sentiment: get_sentiment_label(sentiment))
df_moderna = df_moderna[df_moderna['sentiment'] != 'neutral']
df_moderna = df_moderna[['lemmatized', 'sentiment']].rename(columns={'lemmatized': 'text'})

df_sinovac.head(20)

df_astrazeneca.head(20)

df_pfizer.head(20)

df_moderna.head(20)

df_sinovac.to_csv('labeled/sinovac_labeled.csv', index=False)
df_astrazeneca.to_csv('labeled/astrazeneca_labeled.csv', index=False)
df_pfizer.to_csv('labeled/pfizer_labeled.csv', index=False)
df_moderna.to_csv('labeled/moderna_labeled.csv', index=False)

"""# **(EDA) Exploratory Data Analysis**"""

df_sinovac = pd.read_csv('labeled/sinovac_labeled.csv')
df_astrazeneca = pd.read_csv('labeled/astrazeneca_labeled.csv')
df_pfizer = pd.read_csv('labeled/pfizer_labeled.csv')
df_moderna = pd.read_csv('labeled/moderna_labeled.csv')

# Retrieve the sentiment class distribution for Sinovac dataset
df_sinovac_agg = df_sinovac.groupby('sentiment').count()
df_sinovac_agg = df_sinovac_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for AstraZeneca dataset
df_astrazeneca_agg = df_astrazeneca.groupby('sentiment').count()
df_astrazeneca_agg = df_astrazeneca_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for Pfizer dataset
df_pfizer_agg = df_pfizer.groupby('sentiment').count()
df_pfizer_agg = df_pfizer_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for Moderna dataset
df_moderna_agg = df_moderna.groupby('sentiment').count()
df_moderna_agg = df_moderna_agg.rename(columns={'text': 'count'})

class ClassDistributionVisualization:
    # Add text inside each bar
    def add_value_label(self, x_list, y_list):
        for index in range(0, len(x_list)):
            plt.text(index, y_list[index-1]/2, y_list[index-1], ha='center', va='center')

    # Visualize the sentiment class distribution with bar plot
    def visualize_bar_data(self,
                           sinovac_sentiment_count,
                           astrazeneca_sentiment_count,
                           pfizer_sentiment_count,
                           moderna_sentiment_count,
                           file_name):
        labels = ['positive', 'negative']

        plt.figure(figsize=(15, 10))
        plt.subplots_adjust(left=0.1,
                            bottom=0.1, 
                            right=0.9, 
                            top=0.9, 
                            wspace=0.4, 
                            hspace=0.4)

        plt.subplot(2, 2, 1)
        plt.bar(labels, sinovac_sentiment_count, color=['tab:green', 'tab:red'])
        self.add_value_label(labels, sinovac_sentiment_count)
        plt.title('SINOVAC DATASET')           
        plt.xlabel('SENTIMENTS')
        plt.ylabel('COUNT')

        plt.subplot(2, 2, 2)
        plt.bar(labels, astrazeneca_sentiment_count, color=['tab:green', 'tab:red'])  
        self.add_value_label(labels, astrazeneca_sentiment_count)
        plt.title('ASTRAZENECA DATASET')           
        plt.xlabel('SENTIMENTS')
        plt.ylabel('COUNT')

        plt.subplot(2, 2, 3)
        plt.bar(labels, pfizer_sentiment_count, color=['tab:green', 'tab:red'])      
        self.add_value_label(labels, pfizer_sentiment_count)
        plt.title('PFIZER DATASET')           
        plt.xlabel('SENTIMENTS')
        plt.ylabel('COUNT')


        plt.subplot(2, 2, 4)
        plt.bar(labels, moderna_sentiment_count, color=['tab:green', 'tab:red']) 
        self.add_value_label(labels, moderna_sentiment_count)
        plt.title('MODERNA DATASET')           
        plt.xlabel('SENTIMENTS')
        plt.ylabel('COUNT')

        plt.savefig(os.path.join('assets/', file_name))
        plt.show()
    
    def visualize_text_length_distribution(self,
                                           df_sinovac,
                                           df_astrazeneca,
                                           df_pfizer,
                                           df_moderna,
                                           file_name):
        plt.figure(figsize=(15, 10))
        plt.subplots_adjust(left=0.1,
                            bottom=0.1, 
                            right=0.9, 
                            top=0.9, 
                            wspace=0.4, 
                            hspace=0.4)

        plt.subplot(2, 2, 1)
        df_sinovac['text_length'].plot(kind='hist',
                                       bins=130,
                                       title='TEXT LENGTH DISTRIBUTION SINOVAC')

        plt.subplot(2, 2, 2)
        df_astrazeneca['text_length'].plot(kind='hist',
                                           bins=130,
                                           title='TEXT LENGTH DISTRIBUTION ASTRAZENECA')

        plt.subplot(2, 2, 3)
        df_pfizer['text_length'].plot(kind='hist',
                                      bins=130,
                                      title='TEXT LENGTH DISTRIBUTION PFIZER')


        plt.subplot(2, 2, 4)
        df_moderna['text_length'].plot(kind='hist',
                                       bins=130,
                                       title='TEXT LENGTH DISTRIBUTION MODERNA')

        plt.savefig(os.path.join('assets/', file_name))
        plt.show()
        
    def visualize_word_count_distribution(self,
                                          df_sinovac,
                                          df_astrazeneca,
                                          df_pfizer,
                                          df_moderna,
                                          file_name):
        plt.figure(figsize=(15, 10))
        plt.subplots_adjust(left=0.1,
                            bottom=0.1, 
                            right=0.9, 
                            top=0.9, 
                            wspace=0.4, 
                            hspace=0.4)

        plt.subplot(2, 2, 1)
        df_sinovac['word_count'].plot(kind='hist',
                                      bins=130,
                                      title='WORD COUNT DISTRIBUTION SINOVAC')

        plt.subplot(2, 2, 2)
        df_astrazeneca['word_count'].plot(kind='hist',
                                          bins=130,
                                          title='WORD COUNT DISTRIBUTION ASTRAZENECA')

        plt.subplot(2, 2, 3)
        df_pfizer['word_count'].plot(kind='hist',
                                     bins=130,
                                     title='WORD COUNT DISTRIBUTION PFIZER')


        plt.subplot(2, 2, 4)
        df_moderna['word_count'].plot(kind='hist',
                                      bins=130,
                                      title='WORD COUNT DISTRIBUTION MODERNA')

        plt.savefig(os.path.join('assets/', file_name))
        plt.show()
        
    def visualize_wordcloud(self,
                            df_positive,
                            df_negative,
                            title,
                            file_name):
        
        plt.figure(figsize=(30, 30))
        
        positive_corpus = ' '.join(text for text in df_positive['text'].values)
        negative_corpus = ' '.join(text for text in df_negative['text'].values)
        
        wordcloud = WordCloud(width=1200,
                              height=900,
                              max_words=100,
                              collocations = True,
                              background_color='white')
        
        plt.subplot(1, 2, 1)
        plt.imshow(wordcloud.generate(positive_corpus), interpolation='bilinear')
        plt.title(title + ' CORPUS POSITIVE WORD FREQUENT', fontsize=20)
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(wordcloud.generate(negative_corpus), interpolation='bilinear')
        plt.title(title + ' CORPUS NEGATIVE WORD FREQUENT', fontsize=20)
        plt.axis('off')

        plt.savefig(os.path.join('assets/', file_name))
        plt.show()

    
class_visualizer = ClassDistributionVisualization()

"""## **Dataset Visualization Before Split**"""

# Visualize the sentiment class before data split
sinovac_sentiment_count = df_sinovac_agg['count'].values
astrazeneca_sentiment_count = df_astrazeneca_agg['count'].values
pfizer_sentiment_count = df_pfizer_agg['count'].values
moderna_sentiment_count = df_moderna_agg['count'].values

class_visualizer.visualize_bar_data(sinovac_sentiment_count,
                                    astrazeneca_sentiment_count,
                                    pfizer_sentiment_count,
                                    moderna_sentiment_count,
                                    'class_before_split.jpg')

print('Total size Sinovac dataset : ', df_sinovac.shape)
print('Total size AstraZeneca dataset : ', df_astrazeneca.shape)
print('Total size Pfizer dataset : ', df_pfizer.shape)
print('Total size Moderna dataset : ', df_moderna.shape)

"""## **Data Distribution**"""

df_sinovac_temp = df_sinovac.copy()
df_astrazeneca_temp = df_astrazeneca.copy()
df_pfizer_temp = df_pfizer.copy()
df_moderna_temp = df_moderna.copy()

df_sinovac_temp['text_length'] = df_sinovac_temp['text'].apply(len)
df_sinovac_temp['word_count'] = df_sinovac_temp['text'].apply(lambda x: len(x.split()))

df_astrazeneca_temp['text_length'] = df_astrazeneca_temp['text'].apply(len)
df_astrazeneca_temp['word_count'] = df_astrazeneca_temp['text'].apply(lambda x: len(x.split()))

df_pfizer_temp['text_length'] = df_pfizer_temp['text'].apply(len)
df_pfizer_temp['word_count'] = df_pfizer_temp['text'].apply(lambda x: len(x.split()))

df_moderna_temp['text_length'] = df_moderna_temp['text'].apply(len)
df_moderna_temp['word_count'] = df_moderna_temp['text'].apply(lambda x: len(x.split()))

class_visualizer.visualize_text_length_distribution(df_sinovac_temp,
                                                    df_astrazeneca_temp,
                                                    df_pfizer_temp,
                                                    df_moderna_temp,
                                                    'text_length_distribution.jpg')

class_visualizer.visualize_word_count_distribution(df_sinovac_temp,
                                                    df_astrazeneca_temp,
                                                    df_pfizer_temp,
                                                    df_moderna_temp,
                                                    'word_count_distribution.jpg')

"""## **Remove Short Text**"""

df_sinovac = df_sinovac_temp[df_sinovac_temp['word_count'] >= 5]
df_astrazeneca = df_astrazeneca_temp[df_astrazeneca_temp['word_count'] >= 5]
df_pfizer = df_pfizer_temp[df_pfizer_temp['word_count'] >= 5]
df_moderna = df_moderna_temp[df_moderna_temp['word_count'] >= 5]

df_sinovac_temp = df_sinovac.copy()
df_astrazeneca_temp = df_astrazeneca.copy()
df_pfizer_temp = df_pfizer.copy()
df_moderna_temp = df_moderna.copy()

class_visualizer.visualize_text_length_distribution(df_sinovac_temp,
                                                    df_astrazeneca_temp,
                                                    df_pfizer_temp,
                                                    df_moderna_temp,
                                                    'text_length_no_short_distribution.jpg')

class_visualizer.visualize_word_count_distribution(df_sinovac_temp,
                                                    df_astrazeneca_temp,
                                                    df_pfizer_temp,
                                                    df_moderna_temp,
                                                    'word_count_no_short_distribution.jpg')

"""## **Word Cloud Visualization**"""

df_sinovac_temp = df_sinovac.copy()
df_astrazeneca_temp = df_astrazeneca.copy()
df_pfizer_temp = df_pfizer.copy()
df_moderna_temp = df_moderna.copy()

df_sinovac_positive_temp = df_sinovac_temp[df_sinovac_temp['sentiment'] == 'positive']
df_sinovac_negative_temp = df_sinovac_temp[df_sinovac_temp['sentiment'] == 'negative']

df_astrazeneca_positive_temp = df_astrazeneca_temp[df_astrazeneca_temp['sentiment'] == 'positive']
df_astrazeneca_negative_temp = df_astrazeneca_temp[df_astrazeneca_temp['sentiment'] == 'negative']

df_pfizer_positive_temp = df_pfizer_temp[df_pfizer_temp['sentiment'] == 'positive']
df_pfizer_negative_temp = df_pfizer_temp[df_pfizer_temp['sentiment'] == 'negative']

df_moderna_positive_temp = df_moderna_temp[df_moderna_temp['sentiment'] == 'positive']
df_moderna_negative_temp = df_moderna_temp[df_moderna_temp['sentiment'] == 'negative']

class_visualizer.visualize_wordcloud(df_sinovac_positive_temp,
                                     df_sinovac_negative_temp,
                                     'SINOVAC',
                                     'word_cloud_sinovac.jpg')

class_visualizer.visualize_wordcloud(df_astrazeneca_positive_temp,
                                     df_astrazeneca_negative_temp,
                                     'ASTRAZENECA',
                                     'word_cloud_astrazeneca.jpg')

class_visualizer.visualize_wordcloud(df_pfizer_positive_temp,
                                     df_pfizer_negative_temp,
                                     'PFIZER',
                                     'word_cloud_pfizer.jpg')

class_visualizer.visualize_wordcloud(df_moderna_positive_temp,
                                     df_moderna_negative_temp,
                                     'MODERNA',
                                     'word_cloud_moderna.jpg')

df_sinovac = df_sinovac_temp[['text', 'sentiment']]
df_astrazeneca = df_astrazeneca_temp[['text', 'sentiment']]
df_pfizer = df_pfizer_temp[['text', 'sentiment']]
df_moderna = df_moderna_temp[['text', 'sentiment']]

print('Total size Sinovac after remove short text dataset : ', df_sinovac.shape)
print('Total size AstraZeneca after remove short text dataset : ', df_astrazeneca.shape)
print('Total size Pfizer after remove short text dataset : ', df_pfizer.shape)
print('Total size Moderna after remove short text dataset : ', df_moderna.shape)

"""## **Dataset Splitting**"""

def split_data(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        train_size=0.8, 
                                                        test_size=0.2, 
                                                        stratify=y,
                                                        shuffle=True,
                                                        random_state=0)


    return X_train, X_test, y_train, y_test

"""### **Sinovac Dataset**"""

# Split the Sinovac dataset into training, validation, and testing
X_sinovac = df_sinovac['text'].values
y_sinovac = df_sinovac['sentiment'].values

X_sinovac_train, X_sinovac_test, y_sinovac_train, y_sinovac_test = split_data(X_sinovac, y_sinovac)

df_sinovac_train = pd.DataFrame({'text': X_sinovac_train, 'sentiment': y_sinovac_train})
df_sinovac_test = pd.DataFrame({'text': X_sinovac_test, 'sentiment': y_sinovac_test})

"""### **AstraZeneca Dataset**"""

# Split the AstraZeneca dataset into training, validation, and testing
X_astrazeneca = df_astrazeneca['text'].values
y_astrazeneca = df_astrazeneca['sentiment'].values

X_astrazeneca_train, X_astrazeneca_test, y_astrazeneca_train, y_astrazeneca_test = split_data(X_astrazeneca, y_astrazeneca)

df_astrazeneca_train = pd.DataFrame({'text': X_astrazeneca_train, 'sentiment': y_astrazeneca_train})
df_astrazeneca_test = pd.DataFrame({'text': X_astrazeneca_test, 'sentiment': y_astrazeneca_test})

"""### **Pfizer Dataset**"""

# Split the Pfizer dataset into training, validation, and testing
X_pfizer = df_pfizer['text'].values
y_pfizer = df_pfizer['sentiment'].values

X_pfizer_train, X_pfizer_test, y_pfizer_train, y_pfizer_test = split_data(X_pfizer, y_pfizer)

df_pfizer_train = pd.DataFrame({'text': X_pfizer_train, 'sentiment': y_pfizer_train})
df_pfizer_test = pd.DataFrame({'text': X_pfizer_test, 'sentiment': y_pfizer_test})

"""### **Moderna Dataset**"""

# Split the Pfizer dataset into training, validation, and testing
X_moderna = df_moderna['text'].values
y_moderna = df_moderna['sentiment'].values

X_moderna_train, X_moderna_test, y_moderna_train, y_moderna_test = split_data(X_moderna, y_moderna)

df_moderna_train = pd.DataFrame({'text': X_moderna_train, 'sentiment': y_moderna_train})
df_moderna_test = pd.DataFrame({'text': X_moderna_test, 'sentiment': y_moderna_test})

df_sinovac_train.to_csv('train/sinovac_train.csv', index=False)
df_astrazeneca_train.to_csv('train/astrazeneca_train.csv', index=False)
df_pfizer_train.to_csv('train/pfizer_train.csv', index=False)
df_moderna_train.to_csv('train/moderna_train.csv', index=False)

df_sinovac_test.to_csv('test/sinovac_test.csv', index=False)
df_astrazeneca_test.to_csv('test/astrazeneca_test.csv', index=False)
df_pfizer_test.to_csv('test/pfizer_test.csv', index=False)
df_moderna_test.to_csv('test/moderna_test.csv', index=False)

"""## **Dataset Visualization After Split**"""

df_sinovac_train = pd.read_csv('train/sinovac_train.csv')
df_astrazeneca_train = pd.read_csv('train/astrazeneca_train.csv')
df_pfizer_train = pd.read_csv('train/pfizer_train.csv')
df_moderna_train = pd.read_csv('train/moderna_train.csv')

df_sinovac_test = pd.read_csv('test/sinovac_test.csv')
df_astrazeneca_test = pd.read_csv('test/astrazeneca_test.csv')
df_pfizer_test = pd.read_csv('test/pfizer_test.csv')
df_moderna_test = pd.read_csv('test/moderna_test.csv')

# Retrieve the sentiment class distribution for Sinovac training data
df_sinovac_train_agg = df_sinovac_train.groupby('sentiment').count()
df_sinovac_train_agg = df_sinovac_train_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for AstraZeneca training data
df_astrazeneca_train_agg = df_astrazeneca_train.groupby('sentiment').count()
df_astrazeneca_train_agg = df_astrazeneca_train_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for Pfizer training data
df_pfizer_train_agg = df_pfizer_train.groupby('sentiment').count()
df_pfizer_train_agg = df_pfizer_train_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for Moderna training data
df_moderna_train_agg = df_moderna_train.groupby('sentiment').count()
df_moderna_train_agg = df_moderna_train_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for Sinovac testing data
df_sinovac_test_agg = df_sinovac_test.groupby('sentiment').count()
df_sinovac_test_agg = df_sinovac_test_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for Moderna AstraZeneca data
df_astrazeneca_test_agg = df_astrazeneca_test.groupby('sentiment').count()
df_astrazeneca_test_agg = df_astrazeneca_test_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for Moderna Pfizer data
df_pfizer_test_agg = df_pfizer_test.groupby('sentiment').count()
df_pfizer_test_agg = df_pfizer_test_agg.rename(columns={'text': 'count'})

# Retrieve the sentiment class distribution for Moderna testing data
df_moderna_test_agg = df_moderna_test.groupby('sentiment').count()
df_moderna_test_agg = df_moderna_test_agg.rename(columns={'text': 'count'})

"""### **Train Data**"""

# Visualize the sentiment class in training data
sinovac_train_sentiment_count = df_sinovac_train_agg['count'].values
astrazeneca_train_sentiment_count = df_astrazeneca_train_agg['count'].values
pfizer_train_sentiment_count = df_pfizer_train_agg['count'].values
moderna_train_sentiment_count = df_moderna_train_agg['count'].values

class_visualizer.visualize_bar_data(sinovac_train_sentiment_count, 
                                    astrazeneca_train_sentiment_count, 
                                    pfizer_train_sentiment_count, 
                                    moderna_train_sentiment_count,
                                    'train_class.jpg')

print('Total training size Sinovac dataset : ', df_sinovac_train.shape)
print('Total training size AstraZeneca dataset : ', df_astrazeneca_train.shape)
print('Total training size Pfizer dataset : ', df_pfizer_train.shape)
print('Total training size Moderna dataset : ', df_moderna_train.shape)

"""### **Distribution**"""

df_sinovac_train_temp = df_sinovac_train.copy()
df_astrazeneca_train_temp = df_astrazeneca_train.copy()
df_pfizer_train_temp = df_pfizer_train.copy()
df_moderna_train_temp = df_moderna_train.copy()

df_sinovac_train_temp['text_length'] = df_sinovac_train_temp['text'].apply(len)
df_sinovac_train_temp['word_count'] = df_sinovac_train_temp['text'].apply(lambda x: len(x.split()))

df_astrazeneca_train_temp['text_length'] = df_astrazeneca_train_temp['text'].apply(len)
df_astrazeneca_train_temp['word_count'] = df_astrazeneca_train_temp['text'].apply(lambda x: len(x.split()))

df_pfizer_train_temp['text_length'] = df_pfizer_train_temp['text'].apply(len)
df_pfizer_train_temp['word_count'] = df_pfizer_train_temp['text'].apply(lambda x: len(x.split()))

df_moderna_train_temp['text_length'] = df_moderna_train_temp['text'].apply(len)
df_moderna_train_temp['word_count'] = df_moderna_train_temp['text'].apply(lambda x: len(x.split()))

class_visualizer.visualize_text_length_distribution(df_sinovac_train_temp,
                                                    df_astrazeneca_train_temp,
                                                    df_pfizer_train_temp,
                                                    df_moderna_train_temp,
                                                    'text_length_train_distribution.jpg')

class_visualizer.visualize_word_count_distribution(df_sinovac_train_temp,
                                                    df_astrazeneca_train_temp,
                                                    df_pfizer_train_temp,
                                                    df_moderna_train_temp,
                                                    'word_count_train_distribution.jpg')

"""### **Test Data**"""

# Visualize the sentiment class in testing data
sinovac_test_sentiment_count = df_sinovac_test_agg['count'].values
astrazeneca_test_sentiment_count = df_astrazeneca_test_agg['count'].values
pfizer_test_sentiment_count = df_pfizer_test_agg['count'].values
moderna_test_sentiment_count = df_moderna_test_agg['count'].values

class_visualizer.visualize_bar_data(sinovac_test_sentiment_count, 
                                    astrazeneca_test_sentiment_count, 
                                    pfizer_test_sentiment_count, 
                                    moderna_test_sentiment_count,
                                    'test_class.jpg')

print('Total test size Sinovac dataset : ', df_sinovac_test.shape)
print('Total test size AstraZeneca dataset : ', df_astrazeneca_test.shape)
print('Total test size Pfizer dataset : ', df_pfizer_test.shape)
print('Total test size Moderna dataset : ', df_moderna_test.shape)

"""### **Distribution**"""

df_sinovac_test_temp = df_sinovac_test.copy()
df_astrazeneca_test_temp = df_astrazeneca_test.copy()
df_pfizer_test_temp = df_pfizer_test.copy()
df_moderna_test_temp = df_moderna_test.copy()

df_sinovac_test_temp['text_length'] = df_sinovac_test_temp['text'].apply(len)
df_sinovac_test_temp['word_count'] = df_sinovac_test_temp['text'].apply(lambda x: len(x.split()))

df_astrazeneca_test_temp['text_length'] = df_astrazeneca_test_temp['text'].apply(len)
df_astrazeneca_test_temp['word_count'] = df_astrazeneca_test_temp['text'].apply(lambda x: len(x.split()))

df_pfizer_test_temp['text_length'] = df_pfizer_test_temp['text'].apply(len)
df_pfizer_test_temp['word_count'] = df_pfizer_test_temp['text'].apply(lambda x: len(x.split()))

df_moderna_test_temp['text_length'] = df_moderna_test_temp['text'].apply(len)
df_moderna_test_temp['word_count'] = df_moderna_test_temp['text'].apply(lambda x: len(x.split()))

class_visualizer.visualize_text_length_distribution(df_sinovac_test_temp,
                                                    df_astrazeneca_test_temp,
                                                    df_pfizer_test_temp,
                                                    df_moderna_test_temp,
                                                    'text_length_test_distribution.jpg')

class_visualizer.visualize_word_count_distribution(df_sinovac_test_temp,
                                                    df_astrazeneca_test_temp,
                                                    df_pfizer_test_temp,
                                                    df_moderna_test_temp,
                                                    'word_count_test_distribution.jpg')

"""# **Data Augmentation**"""

df_sinovac_train = pd.read_csv('train/sinovac_train.csv')
df_astrazeneca_train = pd.read_csv('train/astrazeneca_train.csv')
df_pfizer_train = pd.read_csv('train/pfizer_train.csv')
df_moderna_train = pd.read_csv('train/moderna_train.csv')

class TextAugmentation:
    def __init__(self):
        self.augmenter = EDA()

    def replace_synonym(self, text):
        augmented_text_portion = max(1, int(len(text)*0.1)) 
        synonym_replaced = self.augmenter.synonym_replacement(text, n=augmented_text_portion)
        return synonym_replaced

    def random_insert(self, text):
        augmented_text_portion = max(1, int(len(text)*0.1)) 
        random_inserted = self.augmenter.random_insertion(text, n=augmented_text_portion)
        return random_inserted

    def random_swap(self, text):
        augmented_text_portion = max(1, int(len(text)*0.1)) 
        random_swaped = self.augmenter.random_swap(text, n=augmented_text_portion)
        return random_swaped

    def random_delete(self, text):
        random_deleted = self.augmenter.random_deletion(text, p=0.1)
        return random_deleted

text_augmentation = TextAugmentation()

"""## **Sinovac Dataset Augmentation**"""

# Sinovac training data before augmentation for each sentiment class
df_sinovac_train_positive = df_sinovac_train[df_sinovac_train['sentiment'] == 'positive']
df_sinovac_train_negative = df_sinovac_train[df_sinovac_train['sentiment'] == 'negative']
print('Total training size before augmentation : ', df_sinovac_train.shape)
print('Total positive size before augmentation : ', df_sinovac_train_positive.shape)
print('Total negative size before augmentation : ', df_sinovac_train_negative.shape)

# Data augmentation for positive class in Sinovac training data
synonym_replacement_train_positive = df_sinovac_train_positive['text'].apply(lambda text: text_augmentation.replace_synonym(text))
random_insertion_train_positive = df_sinovac_train_positive['text'].apply(lambda text: text_augmentation.random_insert(text))
random_swap_train_positive = df_sinovac_train_positive['text'].apply(lambda text: text_augmentation.random_swap(text))
random_deletion_train_positive = df_sinovac_train_positive['text'].apply(lambda text: text_augmentation.random_delete(text))

# Data augmentation for negative class in Sinovac training data 
synonym_replacement_train_negative = df_sinovac_train_negative['text'].apply(lambda text: text_augmentation.replace_synonym(text))
random_insertion_train_negative = df_sinovac_train_negative['text'].apply(lambda text: text_augmentation.random_insert(text))
random_swap_train_negative = df_sinovac_train_negative['text'].apply(lambda text: text_augmentation.random_swap(text))
random_deletion_train_negative = df_sinovac_train_negative['text'].apply(lambda text: text_augmentation.random_delete(text))

# Merged each augmented positive class data and set the sentiment class to positive sentiment for Sinovac training data
augmented_train_positive = synonym_replacement_train_positive.append([random_insertion_train_positive, 
                                                                      random_swap_train_positive, 
                                                                      random_deletion_train_positive])
df_augmented_train_positive = pd.DataFrame({'text': augmented_train_positive})
df_augmented_train_positive['sentiment'] = 'positive'

# Merged each augmented negative class data and set the sentiment class to negative sentiment for Sinovac training data
augmented_train_negative = synonym_replacement_train_negative.append([random_insertion_train_negative, 
                                                                      random_swap_train_negative, 
                                                                      random_deletion_train_negative])
df_augmented_train_negative = pd.DataFrame({'text': augmented_train_negative})
df_augmented_train_negative['sentiment'] = 'negative'

# Merged all the augmented data with the original data for Sinovac training data
df_sinovac_train = pd.concat([df_sinovac_train, df_augmented_train_positive, df_augmented_train_negative], ignore_index=True)

df_sinovac_train_positive = df_sinovac_train[df_sinovac_train['sentiment'] == 'positive']
df_sinovac_train_negative = df_sinovac_train[df_sinovac_train['sentiment'] == 'negative']

# Sinovac training data after augmentation for each sentiment class
print('Total training size after agumentation : ', df_sinovac_train.shape)
print('Total positive size after agumentation : ', df_sinovac_train_positive.shape)
print('Total negative size after agumentation : ', df_sinovac_train_negative.shape)

"""## **AstraZeneca Dataset Augmentation**"""

# AstraZeneca training data before augmentation for each sentiment class
df_astrazeneca_train_positive = df_astrazeneca_train[df_astrazeneca_train['sentiment'] == 'positive']
df_astrazeneca_train_negative = df_astrazeneca_train[df_astrazeneca_train['sentiment'] == 'negative']
print('Total training size before augmentation : ', df_astrazeneca_train.shape)
print('Total positive size before augmentation : ', df_astrazeneca_train_positive.shape)
print('Total negative size before augmentation : ', df_astrazeneca_train_negative.shape)

# Data augmentation for positive class in AstraZeneca training data
synonym_replacement_train_positive = df_astrazeneca_train_positive['text'].apply(lambda text: text_augmentation.replace_synonym(text))
random_insertion_train_positive = df_astrazeneca_train_positive['text'].apply(lambda text: text_augmentation.random_insert(text))
random_swap_train_positive = df_astrazeneca_train_positive['text'].apply(lambda text: text_augmentation.random_swap(text))
random_deletion_train_positive = df_astrazeneca_train_positive['text'].apply(lambda text: text_augmentation.random_delete(text))

# Data augmentation for negative class in AstraZeneca training data
synonym_replacement_train_negative = df_astrazeneca_train_negative['text'].apply(lambda text: text_augmentation.replace_synonym(text))
random_insertion_train_negative = df_astrazeneca_train_negative['text'].apply(lambda text: text_augmentation.random_insert(text))
random_swap_train_negative = df_astrazeneca_train_negative['text'].apply(lambda text: text_augmentation.random_swap(text))
random_deletion_train_negative = df_astrazeneca_train_negative['text'].apply(lambda text: text_augmentation.random_delete(text))

# Merged each augmented positive class data and set the sentiment class to positive sentiment for AstraZeneca training data
augmented_train_positive = synonym_replacement_train_positive.append([random_insertion_train_positive, 
                                                                      random_swap_train_positive, 
                                                                      random_deletion_train_positive])
df_augmented_train_positive = pd.DataFrame({'text': augmented_train_positive})
df_augmented_train_positive['sentiment'] = 'positive'

# Merged each augmented negative class data and set the sentiment class to negative sentiment for AstraZeneca training data
augmented_train_negative = synonym_replacement_train_negative.append([random_insertion_train_negative, 
                                                                      random_swap_train_negative, 
                                                                      random_deletion_train_negative])
df_augmented_train_negative = pd.DataFrame({'text': augmented_train_negative})
df_augmented_train_negative['sentiment'] = 'negative'

# Merged all the augmented data with the original data for AstraZeneca training data
df_astrazeneca_train = pd.concat([df_astrazeneca_train, df_augmented_train_positive, df_augmented_train_negative], ignore_index=True)

df_astrazeneca_train_positive = df_astrazeneca_train[df_astrazeneca_train['sentiment'] == 'positive']
df_astrazeneca_train_negative = df_astrazeneca_train[df_astrazeneca_train['sentiment'] == 'negative']

# AstraZeneca training data after augmentation for each sentiment class
print('Total training size after agumentation : ', df_astrazeneca_train.shape)
print('Total positive size after agumentation : ', df_astrazeneca_train_positive.shape)
print('Total negative size after agumentation : ', df_astrazeneca_train_negative.shape)

"""## **Pfizer Dataset Augmentation**"""

# Pfizer training data after augmentation for each sentiment class
df_pfizer_train_positive = df_pfizer_train[df_pfizer_train['sentiment'] == 'positive']
df_pfizer_train_negative = df_pfizer_train[df_pfizer_train['sentiment'] == 'negative']
print('Total training size before augmentation : ', df_pfizer_train.shape)
print('Total positive size before augmentation : ', df_pfizer_train_positive.shape)
print('Total negative size before augmentation : ', df_pfizer_train_negative.shape)

# Data augmentation for positive class in Pfizer training data
synonym_replacement_train_positive = df_pfizer_train_positive['text'].apply(lambda text: text_augmentation.replace_synonym(text))
random_insertion_train_positive = df_pfizer_train_positive['text'].apply(lambda text: text_augmentation.random_insert(text))
random_swap_train_positive = df_pfizer_train_positive['text'].apply(lambda text: text_augmentation.random_swap(text))
random_deletion_train_positive = df_pfizer_train_positive['text'].apply(lambda text: text_augmentation.random_delete(text))

# Data augmentation for negative class in Pfizer training data
synonym_replacement_train_negative = df_pfizer_train_negative['text'].apply(lambda text: text_augmentation.replace_synonym(text))
random_insertion_train_negative = df_pfizer_train_negative['text'].apply(lambda text: text_augmentation.random_insert(text))
random_swap_train_negative = df_pfizer_train_negative['text'].apply(lambda text: text_augmentation.random_swap(text))
random_deletion_train_negative = df_pfizer_train_negative['text'].apply(lambda text: text_augmentation.random_delete(text))

# Merged each augmented positive class data and set the sentiment class to positive sentiment for Pfizer training data
augmented_train_positive = synonym_replacement_train_positive.append([random_insertion_train_positive, 
                                                                      random_swap_train_positive, 
                                                                      random_deletion_train_positive])
df_augmented_train_positive = pd.DataFrame({'text': augmented_train_positive})
df_augmented_train_positive['sentiment'] = 'positive'

# Merged each augmented negative class data and set the sentiment class to negative sentiment for Pfizer training data
augmented_train_negative = synonym_replacement_train_negative.append([random_insertion_train_negative, 
                                                                      random_swap_train_negative, 
                                                                      random_deletion_train_negative])
df_augmented_train_negative = pd.DataFrame({'text': augmented_train_negative})
df_augmented_train_negative['sentiment'] = 'negative'

# Merged all the augmented data with the original data for Pfizer training data
df_pfizer_train = pd.concat([df_pfizer_train, df_augmented_train_positive, df_augmented_train_negative], ignore_index=True)

df_pfizer_train_positive = df_pfizer_train[df_pfizer_train['sentiment'] == 'positive']
df_pfizer_train_negative = df_pfizer_train[df_pfizer_train['sentiment'] == 'negative']


# Pfizer training data after augmentation for each sentiment class
print('Total training size after augmentation : ', df_pfizer_train.shape)
print('Total positive size after augmentation : ', df_pfizer_train_positive.shape)
print('Total negative size after augmentation : ', df_pfizer_train_negative.shape)

"""## **Moderna Dataset Augmentation**"""

# Moderna training data after augmentation for each sentiment class
df_moderna_train_positive = df_moderna_train[df_moderna_train['sentiment'] == 'positive']
df_moderna_train_negative = df_moderna_train[df_moderna_train['sentiment'] == 'negative']
print('Total training size before augmentation : ', df_moderna_train.shape)
print('Total positive size before augmentation : ', df_moderna_train_positive.shape)
print('Total negative size before augmentation : ', df_moderna_train_negative.shape)

# Data augmentation for positive class in Moderna training data
synonym_replacement_train_positive = df_moderna_train_positive['text'].apply(lambda text: text_augmentation.replace_synonym(text))
random_insertion_train_positive = df_moderna_train_positive['text'].apply(lambda text: text_augmentation.random_insert(text))
random_swap_train_positive = df_moderna_train_positive['text'].apply(lambda text: text_augmentation.random_swap(text))
random_deletion_train_positive = df_moderna_train_positive['text'].apply(lambda text: text_augmentation.random_delete(text))

# Data augmentation for negative class in Moderna training data
synonym_replacement_train_negative = df_moderna_train_negative['text'].apply(lambda text: text_augmentation.replace_synonym(text))
random_insertion_train_negative = df_moderna_train_negative['text'].apply(lambda text: text_augmentation.random_insert(text))
random_swap_train_negative = df_moderna_train_negative['text'].apply(lambda text: text_augmentation.random_swap(text))
random_deletion_train_negative = df_moderna_train_negative['text'].apply(lambda text: text_augmentation.random_delete(text))

# Merged each augmented positive class data and set the sentiment class to positive sentiment for Moderna training data
augmented_train_positive = synonym_replacement_train_positive.append([random_insertion_train_positive, 
                                                                      random_swap_train_positive, 
                                                                      random_deletion_train_positive])
df_augmented_train_positive = pd.DataFrame({'text': augmented_train_positive})
df_augmented_train_positive['sentiment'] = 'positive'

# Merged each augmented negative class data and set the sentiment class to negative sentiment for Moderna training data
augmented_train_negative = synonym_replacement_train_negative.append([random_insertion_train_negative, 
                                                                      random_swap_train_negative, 
                                                                      random_deletion_train_negative])
df_augmented_train_negative = pd.DataFrame({'text': augmented_train_negative})
df_augmented_train_negative['sentiment'] = 'negative'

# Merged all the augmented data with the original data for Moderna training data
df_moderna_train = pd.concat([df_moderna_train, df_augmented_train_positive, df_augmented_train_negative], ignore_index=True)

df_moderna_train_positive = df_moderna_train[df_moderna_train['sentiment'] == 'positive']
df_moderna_train_negative = df_moderna_train[df_moderna_train['sentiment'] == 'negative']

# Moderna training data after augmentation for each sentiment class
print('Total training size after augmentation : ', df_moderna_train.shape)
print('Total positive size after augmentation : ', df_moderna_train_positive.shape)
print('Total negative size after augmentation : ', df_moderna_train_negative.shape)

df_sinovac_train.to_csv('augmented/sinovac_augmented.csv', index=False)
df_astrazeneca_train.to_csv('augmented/astrazeneca_augmented.csv', index=False)
df_pfizer_train.to_csv('augmented/pfizer_augmented.csv', index=False)
df_moderna_train.to_csv('augmented/moderna_augmented.csv', index=False)

"""# **Data Transformation**"""

df_sinovac_train = pd.read_csv('augmented/sinovac_augmented.csv')
df_astrazeneca_train = pd.read_csv('augmented/astrazeneca_augmented.csv')
df_pfizer_train = pd.read_csv('augmented/pfizer_augmented.csv')
df_moderna_train = pd.read_csv('augmented/moderna_augmented.csv')

df_sinovac_test = pd.read_csv('test/sinovac_test.csv')
df_astrazeneca_test = pd.read_csv('test/astrazeneca_test.csv')
df_pfizer_test = pd.read_csv('test/pfizer_test.csv')
df_moderna_test = pd.read_csv('test/moderna_test.csv')

df_sinovac = pd.concat([df_sinovac_train, df_sinovac_test], ignore_index=True)
df_astrazeneca = pd.concat([df_astrazeneca_train, df_astrazeneca_test], ignore_index=True)
df_pfizer = pd.concat([df_pfizer_train, df_pfizer_test], ignore_index=True)
df_moderna = pd.concat([df_moderna_train, df_moderna_test], ignore_index=True)

def encode_label(sentiment):
    if sentiment == 'negative':
        return 0
    elif sentiment == 'positive':
        return 1

"""## **Sinovac Dataset**"""

# Tokenize the sequence
sinovac_max_length = 100
sinovac_tokenizer = Tokenizer()
sinovac_tokenizer.fit_on_texts(df_sinovac['text'].values)

X_sinovac_train_tokenized = sinovac_tokenizer.texts_to_sequences(df_sinovac_train['text'].values)
X_sinovac_test_tokenized = sinovac_tokenizer.texts_to_sequences(df_sinovac_test['text'].values)

# Pad the sequence
X_sinovac_train_padded = pad_sequences(X_sinovac_train_tokenized, maxlen=sinovac_max_length)
X_sinovac_test_padded = pad_sequences(X_sinovac_test_tokenized, maxlen=sinovac_max_length)

sinovac_num_words = len(sinovac_tokenizer.word_index) + 1

# Encode label
df_sinovac_train['sentiment'] = df_sinovac_train['sentiment'].apply(lambda x: encode_label(x))
df_sinovac_test['sentiment'] = df_sinovac_test['sentiment'].apply(lambda x: encode_label(x))

y_sinovac_train_category = df_sinovac_train['sentiment'].values
y_sinovac_test_category = to_categorical(df_sinovac_test['sentiment'])

print('The maximum sequence length : ', sinovac_max_length)
print('Total unique words : ', sinovac_num_words)
print('Padded training data : ', X_sinovac_train_padded.shape)
print('Padded testing data : ', X_sinovac_test_padded.shape)
print('Train label size : ', y_sinovac_train_category.shape)
print('Test label size : ', y_sinovac_test_category.shape)

"""## **AstraZeneca Dataset**"""

# Tokenize the sequence
astrazeneca_max_length = 100
astrazeneca_tokenizer = Tokenizer()
astrazeneca_tokenizer.fit_on_texts(df_astrazeneca['text'].values)

X_astrazeneca_train_tokenized = astrazeneca_tokenizer.texts_to_sequences(df_astrazeneca_train['text'].values)
X_astrazeneca_test_tokenized = astrazeneca_tokenizer.texts_to_sequences(df_astrazeneca_test['text'].values)

# Pad the sequence
X_astrazeneca_train_padded = pad_sequences(X_astrazeneca_train_tokenized, maxlen=astrazeneca_max_length)
X_astrazeneca_test_padded = pad_sequences(X_astrazeneca_test_tokenized, maxlen=astrazeneca_max_length)

astrazeneca_num_words = len(astrazeneca_tokenizer.word_index) + 1

# Encode label
df_astrazeneca_train['sentiment'] = df_astrazeneca_train['sentiment'].apply(lambda x: encode_label(x))
df_astrazeneca_test['sentiment'] = df_astrazeneca_test['sentiment'].apply(lambda x: encode_label(x))

y_astrazeneca_train_category = df_astrazeneca_train['sentiment'].values
y_astrazeneca_test_category = to_categorical(df_astrazeneca_test['sentiment'])

print('The maximum sequence length : ', astrazeneca_max_length)
print('Total unique words : ', astrazeneca_num_words)
print('Padded training data : ', X_astrazeneca_train_padded.shape)
print('Padded testing data : ', X_astrazeneca_test_padded.shape)
print('Train label size : ', y_astrazeneca_train_category.shape)
print('Test label size : ', y_astrazeneca_test_category.shape)

"""## **Pfizer Dataset**"""

# Tokenize the sequence
pfizer_max_length = 100
pfizer_tokenizer = Tokenizer()
pfizer_tokenizer.fit_on_texts(df_pfizer['text'].values)

X_pfizer_train_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_train['text'].values)
X_pfizer_test_tokenized = pfizer_tokenizer.texts_to_sequences(df_pfizer_test['text'].values)

# Pad the sequence
X_pfizer_train_padded = pad_sequences(X_pfizer_train_tokenized, maxlen=pfizer_max_length)
X_pfizer_test_padded = pad_sequences(X_pfizer_test_tokenized, maxlen=pfizer_max_length)

pfizer_num_words = len(pfizer_tokenizer.word_index) + 1

# Encode label
df_pfizer_train['sentiment'] = df_pfizer_train['sentiment'].apply(lambda x: encode_label(x))
df_pfizer_test['sentiment'] = df_pfizer_test['sentiment'].apply(lambda x: encode_label(x))

y_pfizer_train_category = df_pfizer_train['sentiment'].values
y_pfizer_test_category = to_categorical(df_pfizer_test['sentiment'])

print('The maximum sequence length : ', pfizer_max_length)
print('Total unique words : ', pfizer_num_words)
print('Padded training data : ', X_pfizer_train_padded.shape)
print('Padded testing data : ', X_pfizer_test_padded.shape)
print('Train label size : ', y_pfizer_train_category.shape)
print('Test label size : ', y_pfizer_test_category.shape)

"""## **Moderna Dataset**"""

# Tokenize the sequence
moderna_max_length = 100
moderna_tokenizer = Tokenizer()
moderna_tokenizer.fit_on_texts(df_moderna['text'].values)

X_moderna_train_tokenized = moderna_tokenizer.texts_to_sequences(df_moderna_train['text'].values)
X_moderna_test_tokenized = moderna_tokenizer.texts_to_sequences(df_moderna_test['text'].values)

# Pad the sequence
X_moderna_train_padded = pad_sequences(X_moderna_train_tokenized, maxlen=moderna_max_length)
X_moderna_test_padded = pad_sequences(X_moderna_test_tokenized, maxlen=moderna_max_length)

moderna_num_words = len(moderna_tokenizer.word_index) + 1

# Encode label
df_moderna_train['sentiment'] = df_moderna_train['sentiment'].apply(lambda x: encode_label(x))
df_moderna_test['sentiment'] = df_moderna_test['sentiment'].apply(lambda x: encode_label(x))

y_moderna_train_category = df_moderna_train['sentiment'].values
y_moderna_test_category = to_categorical(df_moderna_test['sentiment'])

print('The maximum sequence length : ', moderna_max_length)
print('Total unique words : ', moderna_num_words)
print('Padded training data : ', X_moderna_train_padded.shape)
print('Padded testing data : ', X_moderna_test_padded.shape)
print('Train label size : ', y_moderna_train_category.shape)
print('Test label size : ', y_moderna_test_category.shape)

"""# **Build Sentiment Analysis Model**

## **Load Pre-Trained Word2Vec**
"""

def load_word_embedding(file_name):
    embeddings_weight_vector = {}
    
    with open(os.path.join('', file_name)) as file:
        for line in file:
            values = line.split();
            word = values[0]
            weights = np.asarray(values[1:], dtype='float32')
            embeddings_weight_vector[word] = weights;
            
    return embeddings_weight_vector

def map_word_embedding(word_index, num_words, embedding_weight_vectors):
    embedding_matrix = np.zeros((num_words, 200))
    for word, index in word_index.items():
        embedding_vector = embedding_weight_vectors.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector

    return embedding_matrix

# Build hybrid CNN-LSTM model
def build_cnn_lstm_model(embedding_matrix, max_sequence_length):
    # Input layer
    input_layer = Input(shape=(max_sequence_length,))

    # Word embedding layer
    embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=True)(input_layer)
    dropout_layer = Dropout(rate=0.5)(embedding_layer)

    # CNN model layer     
    cnn_layer = Conv1D(filters=64,
                       kernel_size=3,
                       strides=1,
                       activation='relu')(dropout_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    cnn_layer = Dropout(rate=0.5)(cnn_layer)
       
    # LSTM model layer
    lstm_layer = LSTM(units=128,
                      dropout=0.5,
                      return_sequences=False)(cnn_layer) 

    # Dense model layer   
    dense_layer = Dense(units=128, activation='relu')(lstm_layer)
    dropout_layer = Dropout(rate=0.5)(dense_layer)
    
    output_layer = Dense(units=2, activation='softmax')(dropout_layer)
    
    cnn_lstm_model = Model(inputs=input_layer, outputs=output_layer)
    
    cnn_lstm_model.compile(loss='categorical_crossentropy',
                           optimizer=Adam(learning_rate=0.001),
                           metrics=['accuracy'])

    return cnn_lstm_model

# Build single CNN model
def build_cnn_model(embedding_matrix, max_sequence_length):
    # Input layer
    input_layer = Input(shape=(max_sequence_length,))

    # Word embedding layer
    embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=True)(input_layer)
    dropout_layer = Dropout(rate=0.5)(embedding_layer)


    # CNN model layer     
    cnn_layer = Conv1D(filters=64,
                       kernel_size=3,
                       strides=1,
                       activation='relu')(dropout_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    cnn_layer = Dropout(rate=0.5)(cnn_layer)
    
    flatten = Flatten()(cnn_layer)

    # Dense model layer   
    dense_layer = Dense(units=128, activation='relu')(flatten)
    dropout_layer = Dropout(rate=0.5)(dense_layer)
    
    output_layer = Dense(units=2, activation='softmax')(dropout_layer)
  
    cnn_model = Model(inputs=input_layer, outputs=output_layer)
    
    cnn_model.compile(loss='categorical_crossentropy',
                      optimizer=Adam(learning_rate=0.001),
                      metrics=['accuracy'])
  
    return cnn_model

# Build single LSTM model
def build_lstm_model(embedding_matrix, max_sequence_length):
    # Input layer
    input_layer = Input(shape=(max_sequence_length,), dtype='int32')
    
    # Word embedding layer
    embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],
                                output_dim=embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                trainable=True)(input_layer)
    dropout_layer = Dropout(rate=0.5)(embedding_layer)

    
    # LSTM model layer
    lstm_layer = LSTM(units=128,
                      dropout=0.5,
                      return_sequences=False)(dropout_layer)    

    # Dense model layer   
    dense_layer = Dense(units=128, activation='relu')(lstm_layer)
    dropout_layer = Dropout(rate=0.5)(dense_layer)
    
    output_layer = Dense(units=2, activation='softmax')(dropout_layer)

    lstm_model = Model(inputs=input_layer, outputs=output_layer)
    
    lstm_model.compile(loss='categorical_crossentropy',
                       optimizer=Adam(learning_rate=0.001),
                       metrics=['accuracy'])

    return lstm_model

# Visualize training result
def visualize_training(histories, file_name):
    
    metrics=['loss', 'accuracy', 'val_accuracy', 'val_loss']
    nrows = (len(metrics)-1) // 2 + 1
    
    fig, axes = plt.subplots(nrows=nrows, ncols=2, figsize=(16, 16))
    axes = axes.reshape(nrows, 2)
    
    for index, metric in enumerate(metrics):
        axes[(index+2)//2 - 1, 1 - (index+1)%2].set_title(metric)
        axes[(index+2)//2 - 1, 1 - (index+1)%2].set_xlabel('epochs')
        axes[(index+2)//2 - 1, 1 - (index+1)%2].set_ylabel(metric)
        
        for history in histories:
            axes[(index+2)//2 - 1, 1 - (index+1)%2].plot(history[metric])
            axes[(index+2)//2 - 1, 1 - (index+1)%2].legend([index+1 for index in range(len(histories))])
            
    fig.savefig(os.path.join('assets/', file_name))

GLOVE_WORD_EMBEDDING = load_word_embedding('glove.twitter.27B.200d.txt')

kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)

"""## **Sinovac Dataset**"""

# Define the training data, validation data, and test data
X_sinovac_train = X_sinovac_train_padded
X_sinovac_test = X_sinovac_test_padded

y_sinovac_train = y_sinovac_train_category
y_sinovac_test = y_sinovac_test_category 

SINOVAC_WORD_INDEX = sinovac_tokenizer.word_index
SINOVAC_NUM_WORDS = sinovac_num_words
SINOVAC_MAX_SEQUENCE = sinovac_max_length
SINOVAC_EMBEDDING_MATRIX = map_word_embedding(SINOVAC_WORD_INDEX,
                                              SINOVAC_NUM_WORDS,
                                              GLOVE_WORD_EMBEDDING)

"""### **Training Model: CNN**"""

# Training single CNN model
sinovac_cnn_histories = []
sinovac_cnn_train_scores = []
sinovac_cnn_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_sinovac_train, y_sinovac_train):
        X_train, X_val = X_sinovac_train[train], X_sinovac_train[val]
        y_train, y_val = to_categorical(y_sinovac_train[train]), to_categorical(y_sinovac_train[val])
        
        sinovac_cnn_model = build_cnn_model(SINOVAC_EMBEDDING_MATRIX, SINOVAC_MAX_SEQUENCE)

        sinovac_cnn_history = sinovac_cnn_model.fit(x=X_train,
                                                    y=y_train,
                                                    batch_size=64,
                                                    validation_data=(X_val, y_val),
                                                    epochs=20)
        
        sinovac_cnn_histories.append(sinovac_cnn_history.history)
        sinovac_cnn_train_scores.append(sinovac_cnn_model.evaluate(X_train, y_train, verbose=0))
        sinovac_cnn_val_scores.append(sinovac_cnn_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(sinovac_cnn_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(sinovac_cnn_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(sinovac_cnn_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(sinovac_cnn_val_scores)[:, 0].mean()))
sinovac_cnn_model.summary()

# Visualize training result
visualize_training(sinovac_cnn_histories, 'sinovac_training_cnn.jpg')

"""### **Training Model: LSTM**"""

# Training single LSTM model
sinovac_lstm_histories = []
sinovac_lstm_train_scores = []
sinovac_lstm_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_sinovac_train, y_sinovac_train):
        X_train, X_val = X_sinovac_train[train], X_sinovac_train[val]
        y_train, y_val = to_categorical(y_sinovac_train[train]), to_categorical(y_sinovac_train[val])
        
        sinovac_lstm_model = build_lstm_model(SINOVAC_EMBEDDING_MATRIX, SINOVAC_MAX_SEQUENCE)

        sinovac_lstm_history = sinovac_lstm_model.fit(x=X_train,
                                                      y=y_train,
                                                      batch_size=64,
                                                      validation_data=(X_val, y_val),
                                                      epochs=20)
        
        sinovac_lstm_histories.append(sinovac_lstm_history.history)
        sinovac_lstm_train_scores.append(sinovac_lstm_model.evaluate(X_train, y_train, verbose=0))
        sinovac_lstm_val_scores.append(sinovac_lstm_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(sinovac_lstm_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(sinovac_lstm_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(sinovac_lstm_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(sinovac_lstm_val_scores)[:, 0].mean()))
sinovac_lstm_model.summary()

# Visualize training result
visualize_training(sinovac_lstm_histories, 'sinovac_training_lstm.jpg')

"""### **Training Model: CNN-LSTM**"""

# Training hybrid CNN-LSTM model
sinovac_cnn_lstm_histories = []
sinovac_cnn_lstm_train_scores = []
sinovac_cnn_lstm_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_sinovac_train, y_sinovac_train):
        X_train, X_val = X_sinovac_train[train], X_sinovac_train[val]
        y_train, y_val = to_categorical(y_sinovac_train[train]), to_categorical(y_sinovac_train[val])
        
        sinovac_cnn_lstm_model = build_cnn_lstm_model(SINOVAC_EMBEDDING_MATRIX, SINOVAC_MAX_SEQUENCE)

        sinovac_cnn_lstm_history = sinovac_cnn_lstm_model.fit(x=X_train,
                                                              y=y_train,
                                                              batch_size=64,
                                                              validation_data=(X_val, y_val),
                                                              epochs=20)
        
        sinovac_cnn_lstm_histories.append(sinovac_cnn_lstm_history.history)
        sinovac_cnn_lstm_train_scores.append(sinovac_cnn_lstm_model.evaluate(X_train, y_train, verbose=0))
        sinovac_cnn_lstm_val_scores.append(sinovac_cnn_lstm_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(sinovac_cnn_lstm_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(sinovac_cnn_lstm_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(sinovac_cnn_lstm_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(sinovac_cnn_lstm_val_scores)[:, 0].mean()))
sinovac_cnn_lstm_model.summary()

# Visualize training result
visualize_training(sinovac_cnn_lstm_histories, 'sinovac_training_cnn_lstm.jpg')

"""## **AstraZeneca Dataset**"""

# Define the training data, validation data, and test data
X_astrazeneca_train = X_astrazeneca_train_padded
X_astrazeneca_test = X_astrazeneca_test_padded

y_astrazeneca_train = y_astrazeneca_train_category
y_astrazeneca_test = y_astrazeneca_test_category 

ASTRAZENECA_WORD_INDEX = astrazeneca_tokenizer.word_index
ASTRAZENECA_NUM_WORDS = astrazeneca_num_words
ASTRAZENECA_MAX_SEQUENCE = astrazeneca_max_length
ASTRAZENECA_EMBEDDING_MATRIX = map_word_embedding(ASTRAZENECA_WORD_INDEX,
                                                  ASTRAZENECA_NUM_WORDS,
                                                  GLOVE_WORD_EMBEDDING)

"""### **Training Model: CNN**"""

# Training single CNN model
astrazeneca_cnn_histories = []
astrazeneca_cnn_train_scores = []
astrazeneca_cnn_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_astrazeneca_train, y_astrazeneca_train):
        X_train, X_val = X_astrazeneca_train[train], X_astrazeneca_train[val]
        y_train, y_val = to_categorical(y_astrazeneca_train[train]), to_categorical(y_astrazeneca_train[val])
        
        astrazeneca_cnn_model = build_cnn_model(ASTRAZENECA_EMBEDDING_MATRIX, ASTRAZENECA_MAX_SEQUENCE)

        astrazeneca_cnn_history = astrazeneca_cnn_model.fit(x=X_train,
                                                            y=y_train,
                                                            batch_size=64,
                                                            validation_data=(X_val, y_val),
                                                            epochs=20)

        astrazeneca_cnn_histories.append(astrazeneca_cnn_history.history)
        astrazeneca_cnn_train_scores.append(astrazeneca_cnn_model.evaluate(X_train, y_train, verbose=0))
        astrazeneca_cnn_val_scores.append(astrazeneca_cnn_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(astrazeneca_cnn_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(astrazeneca_cnn_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(astrazeneca_cnn_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(astrazeneca_cnn_val_scores)[:, 0].mean()))
astrazeneca_cnn_model.summary()

# Visualize training result
visualize_training(astrazeneca_cnn_histories, 'astrazeneca_training_cnn.jpg')

"""### **Training Model: LSTM**"""

# Training single LSTM model
astrazeneca_lstm_histories = []
astrazeneca_lstm_train_scores = []
astrazeneca_lstm_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_astrazeneca_train, y_astrazeneca_train):
        X_train, X_val = X_astrazeneca_train[train], X_astrazeneca_train[val]
        y_train, y_val = to_categorical(y_astrazeneca_train[train]), to_categorical(y_astrazeneca_train[val])
        
        astrazeneca_lstm_model = build_lstm_model(ASTRAZENECA_EMBEDDING_MATRIX, ASTRAZENECA_MAX_SEQUENCE)

        astrazeneca_lstm_history = astrazeneca_lstm_model.fit(x=X_train,
                                                              y=y_train,
                                                              batch_size=64,
                                                              validation_data=(X_val, y_val),
                                                              epochs=20)

        astrazeneca_lstm_histories.append(astrazeneca_lstm_history.history)
        astrazeneca_lstm_train_scores.append(astrazeneca_lstm_model.evaluate(X_train, y_train, verbose=0))
        astrazeneca_lstm_val_scores.append(astrazeneca_lstm_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(astrazeneca_lstm_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(astrazeneca_lstm_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(astrazeneca_lstm_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(astrazeneca_lstm_val_scores)[:, 0].mean()))
astrazeneca_lstm_model.summary()

# Visualize training result
visualize_training(astrazeneca_lstm_histories, 'astrazeneca_training_lstm.jpg')

"""### **Training Model: CNN-LSTM**"""

# Training hybrid CNN-LSTM model
astrazeneca_cnn_lstm_histories = []
astrazeneca_cnn_lstm_train_scores = []
astrazeneca_cnn_lstm_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_astrazeneca_train, y_astrazeneca_train):
        X_train, X_val = X_astrazeneca_train[train], X_astrazeneca_train[val]
        y_train, y_val = to_categorical(y_astrazeneca_train[train]), to_categorical(y_astrazeneca_train[val])
        
        astrazeneca_cnn_lstm_model = build_cnn_lstm_model(ASTRAZENECA_EMBEDDING_MATRIX, ASTRAZENECA_MAX_SEQUENCE)

        astrazeneca_cnn_lstm_history = astrazeneca_cnn_lstm_model.fit(x=X_train,
                                                                      y=y_train,
                                                                      batch_size=64,
                                                                      validation_data=(X_val, y_val),
                                                                      epochs=20)

        astrazeneca_cnn_lstm_histories.append(astrazeneca_cnn_lstm_history.history)
        astrazeneca_cnn_lstm_train_scores.append(astrazeneca_cnn_lstm_model.evaluate(X_train, y_train, verbose=0))
        astrazeneca_cnn_lstm_val_scores.append(astrazeneca_cnn_lstm_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(astrazeneca_cnn_lstm_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(astrazeneca_cnn_lstm_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(astrazeneca_cnn_lstm_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(astrazeneca_cnn_lstm_val_scores)[:, 0].mean()))
astrazeneca_cnn_lstm_model.summary()

# Visualize training result
visualize_training(astrazeneca_cnn_lstm_histories, 'astrazeneca_training_cnn_lstm.jpg')

"""## **Pfizer Dataset**"""

# Define the training data, validation data, and test data
X_pfizer_train = X_pfizer_train_padded
X_pfizer_test = X_pfizer_test_padded

y_pfizer_train = y_pfizer_train_category
y_pfizer_test = y_pfizer_test_category 

PFIZER_WORD_INDEX = pfizer_tokenizer.word_index
PFIZER_NUM_WORDS = pfizer_num_words
PFIZER_MAX_SEQUENCE = pfizer_max_length
PFIZER_EMBEDDING_MATRIX = map_word_embedding(PFIZER_WORD_INDEX,
                                             PFIZER_NUM_WORDS,
                                             GLOVE_WORD_EMBEDDING)

"""### **Training Model: CNN**"""

# Training single CNN model
pfizer_cnn_histories = []
pfizer_cnn_train_scores = []
pfizer_cnn_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_pfizer_train, y_pfizer_train):
        X_train, X_val = X_pfizer_train[train], X_pfizer_train[val]
        y_train, y_val = to_categorical(y_pfizer_train[train]), to_categorical(y_pfizer_train[val])
        
        pfizer_cnn_model = build_cnn_model(PFIZER_EMBEDDING_MATRIX, PFIZER_MAX_SEQUENCE)

        pfizer_cnn_history = pfizer_cnn_model.fit(x=X_train,
                                                  y=y_train,
                                                  batch_size=64,
                                                  validation_data=(X_val, y_val),
                                                  epochs=20)

        pfizer_cnn_histories.append(pfizer_cnn_history.history)
        pfizer_cnn_train_scores.append(pfizer_cnn_model.evaluate(X_train, y_train, verbose=0))
        pfizer_cnn_val_scores.append(pfizer_cnn_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(pfizer_cnn_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(pfizer_cnn_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(pfizer_cnn_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(pfizer_cnn_val_scores)[:, 0].mean()))
pfizer_cnn_model.summary()

# Visualize training result
visualize_training(pfizer_cnn_histories, 'pfizer_training_cnn.jpg')

"""### **Training Model: LSTM**"""

# Training single LSTM model
pfizer_lstm_histories = []
pfizer_lstm_train_scores = []
pfizer_lstm_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_pfizer_train, y_pfizer_train):
        X_train, X_val = X_pfizer_train[train], X_pfizer_train[val]
        y_train, y_val = to_categorical(y_pfizer_train[train]), to_categorical(y_pfizer_train[val])
        
        pfizer_lstm_model = build_lstm_model(PFIZER_EMBEDDING_MATRIX, PFIZER_MAX_SEQUENCE)

        pfizer_lstm_history = pfizer_lstm_model.fit(x=X_train,
                                                    y=y_train,
                                                    batch_size=64,
                                                    validation_data=(X_val, y_val),
                                                    epochs=20)

        pfizer_lstm_histories.append(pfizer_lstm_history.history)
        pfizer_lstm_train_scores.append(pfizer_lstm_model.evaluate(X_train, y_train, verbose=0))
        pfizer_lstm_val_scores.append(pfizer_lstm_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(pfizer_lstm_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(pfizer_lstm_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(pfizer_lstm_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(pfizer_lstm_val_scores)[:, 0].mean()))
pfizer_lstm_model.summary()

# Visualize training result
visualize_training(pfizer_lstm_histories, 'pfizer_training_lstm.jpg')

"""### **Training Model: CNN-LSTM**"""

# Training hybrid CNN-LSTM model
pfizer_cnn_lstm_histories = []
pfizer_cnn_lstm_train_scores = []
pfizer_cnn_lstm_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_pfizer_train, y_pfizer_train):
        X_train, X_val = X_pfizer_train[train], X_pfizer_train[val]
        y_train, y_val = to_categorical(y_pfizer_train[train]), to_categorical(y_pfizer_train[val])
        
        pfizer_cnn_lstm_model = build_cnn_lstm_model(PFIZER_EMBEDDING_MATRIX, PFIZER_MAX_SEQUENCE)

        pfizer_cnn_lstm_history = pfizer_cnn_lstm_model.fit(x=X_train,
                                                            y=y_train,
                                                            batch_size=64,
                                                            validation_data=(X_val, y_val),
                                                            epochs=20)

        pfizer_cnn_lstm_histories.append(pfizer_cnn_lstm_history.history)
        pfizer_cnn_lstm_train_scores.append(pfizer_cnn_lstm_model.evaluate(X_train, y_train, verbose=0))
        pfizer_cnn_lstm_val_scores.append(pfizer_cnn_lstm_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(pfizer_cnn_lstm_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(pfizer_cnn_lstm_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(pfizer_cnn_lstm_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(pfizer_cnn_lstm_val_scores)[:, 0].mean()))
pfizer_cnn_lstm_model.summary()

# Visualize training result
visualize_training(pfizer_cnn_lstm_histories, 'pfizer_training_cnn_lstm.jpg')

"""## **Moderna Dataset**"""

# Define the training data, validation data, and test data
X_moderna_train = X_moderna_train_padded
X_moderna_test = X_moderna_test_padded

y_moderna_train = y_moderna_train_category
y_moderna_test = y_moderna_test_category 

MODERNA_WORD_INDEX = moderna_tokenizer.word_index
MODERNA_NUM_WORDS = moderna_num_words
MODERNA_MAX_SEQUENCE = moderna_max_length
MODERNA_EMBEDDING_MATRIX = map_word_embedding(MODERNA_WORD_INDEX,
                                              MODERNA_NUM_WORDS,
                                              GLOVE_WORD_EMBEDDING)

"""### **Training Model: CNN**"""

# Training single CNN model
moderna_cnn_histories = []
moderna_cnn_train_scores = []
moderna_cnn_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_moderna_train, y_moderna_train):
        X_train, X_val = X_moderna_train[train], X_moderna_train[val]
        y_train, y_val = to_categorical(y_moderna_train[train]), to_categorical(y_moderna_train[val])
        
        moderna_cnn_model = build_cnn_model(MODERNA_EMBEDDING_MATRIX, MODERNA_MAX_SEQUENCE)

        moderna_cnn_history = moderna_cnn_model.fit(x=X_train,
                                                    y=y_train,
                                                    batch_size=64,
                                                    validation_data=(X_val, y_val),
                                                    epochs=20)

        moderna_cnn_histories.append(moderna_cnn_history.history)
        moderna_cnn_train_scores.append(moderna_cnn_model.evaluate(X_train, y_train, verbose=0))
        moderna_cnn_val_scores.append(moderna_cnn_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(moderna_cnn_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(moderna_cnn_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(moderna_cnn_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(moderna_cnn_val_scores)[:, 0].mean()))
moderna_cnn_model.summary()

# Visualize training result
visualize_training(moderna_cnn_histories, 'moderna_training_cnn.jpg')

"""### **Training Model: LSTM**"""

# Training single LSTM model
moderna_lstm_histories = []
moderna_lstm_train_scores = []
moderna_lstm_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_moderna_train, y_moderna_train):
        X_train, X_val = X_moderna_train[train], X_moderna_train[val]
        y_train, y_val = to_categorical(y_moderna_train[train]), to_categorical(y_moderna_train[val])
        
        moderna_lstm_model = build_lstm_model(MODERNA_EMBEDDING_MATRIX, MODERNA_MAX_SEQUENCE)

        moderna_lstm_history = moderna_lstm_model.fit(x=X_train,
                                                      y=y_train,
                                                      batch_size=64,
                                                      validation_data=(X_val, y_val),
                                                      epochs=20)

        moderna_lstm_histories.append(moderna_lstm_history.history)
        moderna_lstm_train_scores.append(moderna_lstm_model.evaluate(X_train, y_train, verbose=0))
        moderna_lstm_val_scores.append(moderna_lstm_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(moderna_lstm_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(moderna_lstm_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(moderna_lstm_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(moderna_lstm_val_scores)[:, 0].mean()))
moderna_lstm_model.summary()

# Visualize training result
visualize_training(moderna_lstm_histories, 'moderna_training_lstm.jpg')

"""### **Training Model: CNN-LSTM**"""

# Training hybrid CNN-LSTM model
moderna_cnn_lstm_histories = []
moderna_cnn_lstm_train_scores = []
moderna_cnn_lstm_val_scores = []

with tf.device('/device:GPU:0'):
    k = 1
    for train, val in kfold.split(X_moderna_train, y_moderna_train):
        X_train, X_val = X_moderna_train[train], X_moderna_train[val]
        y_train, y_val = to_categorical(y_moderna_train[train]), to_categorical(y_moderna_train[val])
        
        moderna_cnn_lstm_model = build_cnn_lstm_model(MODERNA_EMBEDDING_MATRIX, MODERNA_MAX_SEQUENCE)

        moderna_cnn_lstm_history = moderna_cnn_lstm_model.fit(x=X_train,
                                                              y=y_train,
                                                              batch_size=64,
                                                              validation_data=(X_val, y_val),
                                                              epochs=20)

        moderna_cnn_lstm_histories.append(moderna_cnn_lstm_history.history)
        moderna_cnn_lstm_train_scores.append(moderna_cnn_lstm_model.evaluate(X_train, y_train, verbose=0))
        moderna_cnn_lstm_val_scores.append(moderna_cnn_lstm_model.evaluate(X_val, y_val, verbose=0))
        print('\n')
        k += 1

print('Average train accuracy: {:.2f}%'.format(np.asarray(moderna_cnn_lstm_train_scores)[:, 1].mean()*100))
print('Average train loss: {:.2f}'.format(np.asarray(moderna_cnn_lstm_train_scores)[:, 0].mean()))
print('Average validation accuracy: {:.2f}%'.format(np.asarray(moderna_cnn_lstm_val_scores)[:, 1].mean()*100))
print('Average validation loss: {:.2f}\n'.format(np.asarray(moderna_cnn_lstm_val_scores)[:, 0].mean()))
moderna_cnn_lstm_model.summary()

# Visualize training result
visualize_training(moderna_cnn_lstm_histories, 'moderna_training_cnn_lstm.jpg')

"""# **Testing & Evaluation Model**"""

def visualize_confussion_matrix(y_true, y_pred, title, file_name):
    
    classes = ['Negative', 'Positive']
    cm = confusion_matrix(y_true, y_pred)
    
    cmap = plt.cm.Blues
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment='center', color='white' if cm[i, j] > thresh else 'black')

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label') 
    

    plt.savefig(os.path.join('assets/', file_name))
    plt.show()
    
def evaluate_model(y_true, y_pred):
    target_names = ['Negative', 'Positive']
    classification_eval = metrics.classification_report(y_true, y_pred, target_names=target_names)
    print(classification_eval)
    
def visualize_metrics(y_pred_cnn_lstm, 
                      y_pred_cnn,
                      y_pred_lstm,
                      y_true,
                      file_name):
    
    accuracies = [accuracy_score(y_true, y_pred_cnn_lstm),
                accuracy_score(y_true, y_pred_cnn),
                accuracy_score(y_true, y_pred_lstm),]
    precissions = [precision_score(y_true, y_pred_cnn_lstm),
                  precision_score(y_true, y_pred_cnn),
                  precision_score(y_true, y_pred_lstm)]
    recalls = [recall_score(y_true, y_pred_cnn_lstm),
              recall_score(y_true, y_pred_cnn),
              recall_score(y_true, y_pred_lstm)]
    f1_scores = [f1_score(y_true, y_pred_cnn_lstm),
                f1_score(y_true, y_pred_cnn),
                f1_score(y_true, y_pred_lstm)]
    models = ['CNN-LSTM', 'CNN', 'LSTM']
    
    df_metrics = pd.DataFrame({'Model': models,
                               'Accuracy': accuracies,
                               'Precision': precissions,
                               'Recall': recalls,
                               'F1-Score': f1_scores})
    
    df_metrics.plot(x="Model",
                    y=["Accuracy", "Precision", "Recall", "F1-Score"],
                    kind="bar",
                    figsize=(15, 10))
    
    plt.savefig(os.path.join('assets/', file_name))
    plt.show()

"""## **Sinovac Dataset**

### **Testing & Evaluation:  CNN**
"""

with tf.device('/device:GPU:0'):
    result = sinovac_cnn_model.evaluate(X_sinovac_test, y_sinovac_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.2}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_sinovac_pred_raw = sinovac_cnn_model.predict(X_sinovac_test)
    y_sinovac_pred_cnn = np.argmax(y_sinovac_pred_raw, axis=1)
    y_sinovac_true = np.argmax(y_sinovac_test, axis=1)

    visualize_confussion_matrix(y_sinovac_pred_cnn,
                                y_sinovac_true,
                                'CNN Model Confussion Matrix',
                                'sinovac_cnn_confussion.jpg')
    
    evaluate_model(y_sinovac_pred_cnn, y_sinovac_true)

sinovac_cnn_model.save('model/sinovac_cnn_model.h5')

"""### **Testing & Evaluation: LSTM**"""

with tf.device('/device:GPU:0'):
    result = sinovac_lstm_model.evaluate(X_sinovac_test, y_sinovac_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.2}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_sinovac_pred_raw = sinovac_lstm_model.predict(X_sinovac_test)
    y_sinovac_pred_lstm = np.argmax(y_sinovac_pred_raw, axis=1)
    y_sinovac_true = np.argmax(y_sinovac_test, axis=1)

    visualize_confussion_matrix(y_sinovac_pred_lstm, 
                                y_sinovac_true,
                                'LSTM Model Confussion Matrix',
                                'sinovac_lstm_confussion.jpg')
    
    evaluate_model(y_sinovac_pred_lstm, y_sinovac_true)

sinovac_lstm_model.save('model/sinovac_lstm_model.h5')

"""
### **Testing & Evaluation:  CNN-LSTM**"""

with tf.device('/device:GPU:0'):
    result = sinovac_cnn_lstm_model.evaluate(X_sinovac_test, y_sinovac_test, verbose=1)

    
print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.2}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_sinovac_pred_raw = sinovac_cnn_lstm_model.predict(X_sinovac_test)
    y_sinovac_pred_cnn_lstm = np.argmax(y_sinovac_pred_raw, axis=1)
    y_sinovac_true = np.argmax(y_sinovac_test, axis=1)

    visualize_confussion_matrix(y_sinovac_pred_cnn_lstm,
                                y_sinovac_true,
                                'CNN-LSTM Model Confussion Matrix',
                                'sinovac_cnn_lstm_confussion.jpg')
    
    evaluate_model(y_sinovac_pred_cnn_lstm, y_sinovac_true)

sinovac_cnn_lstm_model.save('model/sinovac_cnn_lstm_model.h5')

visualize_metrics(y_sinovac_pred_cnn, 
                  y_sinovac_pred_lstm, 
                  y_sinovac_pred_cnn_lstm, 
                  y_sinovac_true,
                  'sinovac_metrics.jpg')

"""## **AstraZeneca Dataset**

### **Testing & Evaluation:  CNN**
"""

with tf.device('/device:GPU:0'):
    result = astrazeneca_cnn_model.evaluate(X_astrazeneca_test, y_astrazeneca_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.2}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_astrazeneca_pred_raw = astrazeneca_cnn_model.predict(X_astrazeneca_test)
    y_astrazeneca_pred_cnn = np.argmax(y_astrazeneca_pred_raw, axis=1)
    y_astrazeneca_true = np.argmax(y_astrazeneca_test, axis=1)

    visualize_confussion_matrix(y_astrazeneca_pred_cnn,
                                y_astrazeneca_true, 
                                'CNN Model Confussion Matrix',
                                'astrazeneca_cnn_confusion.jpg')
    
    evaluate_model(y_astrazeneca_pred_cnn, y_astrazeneca_true)

astrazeneca_cnn_model.save('model/astrazeneca_cnn_model.h5')

"""### **Testing & Evaluation: LSTM**"""

with tf.device('/device:GPU:0'):
    result = astrazeneca_lstm_model.evaluate(X_astrazeneca_test, y_astrazeneca_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.2}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_astrazeneca_pred_raw = astrazeneca_lstm_model.predict(X_astrazeneca_test)
    y_astrazeneca_pred_lstm = np.argmax(y_astrazeneca_pred_raw, axis=1)
    y_astrazeneca_true = np.argmax(y_astrazeneca_test, axis=1)

    visualize_confussion_matrix(y_astrazeneca_pred_lstm,
                                y_astrazeneca_true,
                                'LSTM Model Confussion Matrix',
                                'astrazeneca_lstm_confusion.jpg')
    
    evaluate_model(y_astrazeneca_pred_lstm, y_astrazeneca_true)

astrazeneca_lstm_model.save('model/astrazeneca_lstm_model.h5')

"""### **Testing & Evaluation:  CNN-LSTM**"""

with tf.device('/device:GPU:0'):
    result = astrazeneca_cnn_lstm_model.evaluate(X_astrazeneca_test, y_astrazeneca_test, verbose=1)


print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.2}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_astrazeneca_pred_raw = astrazeneca_cnn_lstm_model.predict(X_astrazeneca_test)
    y_astrazeneca_pred_cnn_lstm = np.argmax(y_astrazeneca_pred_raw, axis=1)
    y_astrazeneca_true = np.argmax(y_astrazeneca_test, axis=1)

    visualize_confussion_matrix(y_astrazeneca_pred_cnn_lstm, 
                                y_astrazeneca_true,
                                'CNN-LSTM Model Confussion Matrix',
                                'astrazeneca_cnn_lstm_confussion.jpg')
    
    evaluate_model(y_astrazeneca_pred_cnn_lstm, y_astrazeneca_true)

astrazeneca_cnn_lstm_model.save('model/astrazeneca_cnn_lstm_model.h5')

visualize_metrics(y_astrazeneca_pred_cnn_lstm, 
                  y_astrazeneca_pred_cnn, 
                  y_astrazeneca_pred_lstm, 
                  y_astrazeneca_true,
                  'astrazeneca_metrics.jpg')

"""## **Pfizer Dataset**

### **Testing & Evaluation:  CNN**
"""

with tf.device('/device:GPU:0'):
    result = pfizer_cnn_model.evaluate(X_pfizer_test, y_pfizer_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.3}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_pfizer_pred_raw = pfizer_cnn_model.predict(X_pfizer_test)
    y_pfizer_pred_cnn = np.argmax(y_pfizer_pred_raw, axis=1)
    y_pfizer_true = np.argmax(y_pfizer_test, axis=1)

    visualize_confussion_matrix(y_pfizer_pred_cnn,
                                y_pfizer_true,
                                'CNN Model Confussion Matrix',
                                'pfizer_cnn_confusion.jpg')
    
    evaluate_model(y_pfizer_pred_cnn, y_pfizer_true)

pfizer_cnn_model.save('model/pfizer_cnn_model.h5')

"""### **Testing & Evaluation: LSTM**"""

with tf.device('/device:GPU:0'):
    result = pfizer_lstm_model.evaluate(X_pfizer_test, y_pfizer_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.3}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_pfizer_pred_raw = pfizer_lstm_model.predict(X_pfizer_test)
    y_pfizer_pred_lstm = np.argmax(y_pfizer_pred_raw, axis=1)
    y_pfizer_true = np.argmax(y_pfizer_test, axis=1)

    visualize_confussion_matrix(y_pfizer_pred_lstm,
                                y_pfizer_true,
                                'LSTM Model Confussion Matrix',
                                'pfizer_lstm_confusion.jpg')
    
    evaluate_model(y_pfizer_pred_lstm, y_pfizer_true)

pfizer_lstm_model.save('model/pfizer_lstm_model.h5')

"""### **Testing & Evaluation:  CNN-LSTM**"""

with tf.device('/device:GPU:0'):
    result = pfizer_cnn_lstm_model.evaluate(X_pfizer_test, y_pfizer_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.3}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_pfizer_pred_raw = pfizer_cnn_lstm_model.predict(X_pfizer_test)
    y_pfizer_pred_cnn_lstm = np.argmax(y_pfizer_pred_raw, axis=1)
    y_pfizer_true = np.argmax(y_pfizer_test, axis=1)

    visualize_confussion_matrix(y_pfizer_pred_cnn_lstm,
                                y_pfizer_true,
                                'CNN-LSTM Model Confussion Matrix',
                                'pfizer_cnn_lstm_confusion.jpg')
    
    evaluate_model(y_pfizer_pred_cnn_lstm, y_pfizer_true)

pfizer_cnn_lstm_model.save('model/pfizer_cnn_lstm_model.h5')

visualize_metrics(y_pfizer_pred_cnn_lstm, 
                  y_pfizer_pred_cnn, 
                  y_pfizer_pred_lstm, 
                  y_pfizer_true,
                  'pfizer_metrics.jpg')

"""## **Moderna Dataset**

### **Testing & Evaluation:  CNN**
"""

with tf.device('/device:GPU:0'):
    result = moderna_cnn_model.evaluate(X_moderna_test, y_moderna_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.3}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_moderna_pred_raw = moderna_cnn_model.predict(X_moderna_test)
    y_moderna_pred_cnn = np.argmax(y_moderna_pred_raw, axis=1)
    y_moderna_true = np.argmax(y_moderna_test, axis=1)

    visualize_confussion_matrix(y_moderna_pred_cnn,
                                y_moderna_true,
                                'CNN Model Confussion Matrix',
                                'moderna_cnn_confusion.jpg')
    
    evaluate_model(y_moderna_pred_cnn, y_moderna_true)

moderna_cnn_model.save('model/moderna_cnn_model.h5')

"""### **Testing & Evaluation: LSTM**"""

with tf.device('/device:GPU:0'):
    result = moderna_lstm_model.evaluate(X_moderna_test, y_moderna_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.3}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_moderna_pred_raw = moderna_lstm_model.predict(X_moderna_test)
    y_moderna_pred_lstm = np.argmax(y_moderna_pred_raw, axis=1)
    y_moderna_true = np.argmax(y_moderna_test, axis=1)

    visualize_confussion_matrix(y_moderna_pred_lstm,
                                y_moderna_true,
                                'LSTM Model Confussion Matrix',
                                'moderna_lstm_confusion.jpg')
    
    evaluate_model(y_moderna_pred_lstm, y_moderna_true)

moderna_lstm_model.save('model/moderna_lstm_model.h5')

"""### **Testing & Evaluation:  CNN-LSTM**"""

with tf.device('/device:GPU:0'):
    result = moderna_cnn_lstm_model.evaluate(X_moderna_test, y_moderna_test, verbose=1)

print('Accuracy test : {:.2f}%'.format(result[1]*100))
print('Loss test : {:.3}'.format(result[0]))

with tf.device('/device:GPU:0'):
    y_moderna_pred_raw = moderna_cnn_lstm_model.predict(X_moderna_test)
    y_moderna_pred_cnn_lstm = np.argmax(y_moderna_pred_raw, axis=1)
    y_moderna_true = np.argmax(y_moderna_test, axis=1)

    visualize_confussion_matrix(y_moderna_pred_cnn_lstm,
                                y_moderna_true,
                                'CNN-LSTM Model Confussion Matrix',
                                'moderna_cnn_lstm_confusion.jpg')
    
    evaluate_model(y_moderna_pred_cnn_lstm, y_moderna_true)

moderna_cnn_lstm_model.save('model/moderna_cnn_lstm_model.h5')

visualize_metrics(y_moderna_pred_cnn_lstm, 
                  y_moderna_pred_cnn, 
                  y_moderna_pred_lstm, 
                  y_moderna_true,
                  'moderna_metrics.jpg')
